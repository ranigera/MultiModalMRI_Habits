{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze DTI - Preprocessing data without TOPUP (for participants for which it cannot be used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages -\n",
    "# -----------------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "import sys\n",
    "import json\n",
    "import platform\n",
    "import socket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "# -----------------------------------------------------------------------------\n",
    "MANUALLY_EXCLUDE_SUBJECTS = []\n",
    "\n",
    "running_on = 'server' if 'Linux' in platform.system() else 'my_mac'\n",
    "\n",
    "if running_on == 'my_mac':\n",
    "    data_path = '/Users/ranigera/Dropbox/DTI_tests'\n",
    "    preproc_path = '/Users/ranigera/Dropbox/DTI_tests/preproc'\n",
    "    dti_path = '/Users/ranigera/Dropbox/DTI_tests/dti'\n",
    "    stats_path = '/Users/ranigera/Dropbox/DTI_tests/stats'\n",
    "    models_path = stats_path + '/models'\n",
    "    launch_files_path = '/Users/ranigera/Dropbox/DTI_analysis/launch_files'\n",
    "else:\n",
    "    data_path = '/export2/DATA/HIS/HIS_server/BIDS'\n",
    "    preproc_path = '/export2/DATA/HIS/HIS_server/analysis/dwi_data/preproc'\n",
    "    dti_path = '/export2/DATA/HIS/HIS_server/analysis/dwi_data/dti'\n",
    "    stats_path = '/export2/DATA/HIS/HIS_server/analysis/dwi_data/stats'\n",
    "    models_path = stats_path + '/models'\n",
    "    launch_files_path = '/export2/DATA/HIS/HIS_server/codes_dwi/launch_files'\n",
    "\n",
    "fmriPrepAnatomyDerivatives_path = data_path + '/derivatives/fmriprep'\n",
    "\n",
    "expectedVolums = {\n",
    "    'AP': 69,\n",
    "    'PA' : 7,\n",
    "    }\n",
    "expectedB0s_indxs = {\n",
    "    'AP_before': [0, 1, 18, 35, 52],\n",
    "    'AP_after': [0, 1, 18, 35, 52]\n",
    "    }\n",
    "\n",
    "n_cores_TOPUP = 2\n",
    "\n",
    "# setting EDDY stuff:\n",
    "EDDY_command = 'eddy_openmp' if running_on == 'server' else 'eddy' # for the boost server\n",
    "EDDY_command = 'eddy_cuda10.2' if running_on == 'server' else 'eddy' # for the cheshire server\n",
    "ssh_command_for_cheshire_server = 'ssh shirangera@cheshire.tau.ac.il' if 'boost' in socket.gethostname() else ''\n",
    "\n",
    "n_expected_EDDY_output_files = 13\n",
    "n_cores_EDDY = 4 # relevant only for running using files (currently disabled as I run it on cheshire's GPU)\n",
    "\n",
    "n_expected_DTIFIT_output_files = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "# -----------------------------------------------------------------------------\n",
    "def createSubjectScansBaseNames(subjFolder, data_path):\n",
    "    sub = int(subjFolder.split(\"-\",1)[1])\n",
    "    group = '1day' if sub < 200 else '3day'\n",
    "    last_sess = group[0]\n",
    "    DWI_path_before = os.path.join(data_path, subjFolder, 'ses-1/dwi/')\n",
    "    DWI_path_after = os.path.join(data_path, subjFolder, f'ses-{last_sess}/dwi/')\n",
    "    scansBaseNames = {\n",
    "        'AP_before': f'{os.path.join(DWI_path_before, \"sub-\" + str(sub) + \"_ses-1_acq-ap_run-01_dwi\")}',\n",
    "        'AP_after' : f'{os.path.join(DWI_path_after, \"sub-\" + str(sub) + \"_ses-\" + last_sess + \"_acq-ap_run-02_dwi\")}'\n",
    "        }\n",
    "    return scansBaseNames\n",
    "\n",
    "def get_sub_B0_files(subjFolder, scansBaseNames, B0s_indxs):\n",
    "    sub_B0s_files = []\n",
    "    for scan in scansBaseNames.keys():\n",
    "        for B0ind in B0s_indxs[scan]:\n",
    "            B0_file_name = os.path.join(preproc_path, subjFolder, subjFolder + '_' + scan + \"_b0_volInd-\" + str(B0ind) + \".nii.gz\")\n",
    "            sub_B0s_files.append(B0_file_name)\n",
    "    return sub_B0s_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Get sub folders\n"
     ]
    }
   ],
   "source": [
    "# Get folders and remove excluded subjects\n",
    "# -----------------------------------------------------------------------------\n",
    "print('>> Get sub folders')\n",
    "subjFolders = [el for el in os.listdir(data_path) if 'sub' in el]\n",
    "\n",
    "if running_on == 'my_mac':\n",
    "    print('>> Get exclusion list')\n",
    "    with open('/Users/ranigera/Google_Drive_TAU/Experiments/HIS_STUDY/Analysis/codes/paths_and_vars.py') as txtFile:\n",
    "        txt = txtFile.read()\n",
    "    participantsToExclude = [int(el) for el in txt.split('participantsToExclude = [')[1].split(']')[0].replace('\\n','').replace('\\n','').replace(\"'\",\"\").split(',')]\n",
    "\n",
    "    print('>> Remove sub folders of excluded participants in case they are there')\n",
    "    subjFolders = [el for el in subjFolders if int(el.split('-')[1]) not in participantsToExclude]\n",
    "\n",
    "if MANUALLY_EXCLUDE_SUBJECTS:\n",
    "    subjFolders = [el for el in subjFolders if int(el.split('-')[1]) not in MANUALLY_EXCLUDE_SUBJECTS]\n",
    "    \n",
    "subjFolders.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for missing scans or wrong phase encoding directions for ALL SUBJECTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Verify that all the scans exist and that the phase encoding directions are as they should.\n",
      " *** Scan not found: /export2/DATA/HIS/HIS_server/BIDS/sub-259/ses-3/dwi/sub-259_ses-3_acq-ap_run-02_dwi.json\n"
     ]
    }
   ],
   "source": [
    "print ('>> Verify that all the scans exist and that the phase encoding directions are as they should.')\n",
    "subjectsWithAProblem = []\n",
    "for subjFolder in subjFolders:\n",
    "    sub = int(subjFolder.split(\"-\",1)[1])\n",
    "    scansBaseNames = createSubjectScansBaseNames(subjFolder, data_path)\n",
    "    for scan in scansBaseNames.keys():\n",
    "        # print(scansBaseNames[scan] + '.json')\n",
    "        # print(scanData['PhaseEncodingDirection'])\n",
    "        if not os.path.exists(scansBaseNames[scan] + '.json'):\n",
    "            subjectsWithAProblem.append(sub)\n",
    "            print(' *** Scan not found: ' + scansBaseNames[scan] + '.json')\n",
    "            continue\n",
    "        with open(scansBaseNames[scan] + '.json') as json_file:        \n",
    "            scanData = json.load(json_file)\n",
    "            if ('acq-ap_' in scansBaseNames[scan] and scanData['PhaseEncodingDirection'] != 'j-') or \\\n",
    "                ('acq-pa_' in scansBaseNames[scan] and scanData['PhaseEncodingDirection'] != 'j'):\n",
    "                subjectsWithAProblem.append(sub)\n",
    "                print(' *** There is a problem with the scanning directions: ' + scansBaseNames[scan] + '.json is defined as ' + scanData['PhaseEncodingDirection'] + '.')\n",
    "                continue\n",
    "\n",
    "subjectsWithAProblem = list(set(subjectsWithAProblem))\n",
    "subjectsWithAProblem.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the other file:\n",
    "UnanalyzableSubjects = [211, 259]\n",
    "#subjToAnalyzeWithOnlyAP = [101, 110, 204, 207, 209, 255]\n",
    "subjToAnalyzeWithOnlyAP = [110, 204, 207, 209, 255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> SELECT SUBJECTS WITH TWO SCANS BUT WITHOUT OPPOSITE PHASE ENCODING\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sub-101', 'sub-110', 'sub-204', 'sub-207', 'sub-209', 'sub-255']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take only subjects that can be analyzed but without opposite phase encoding (PA) scans\n",
    "# -----------------------------------------------------------------------------\n",
    "print('>> SELECT SUBJECTS WITH TWO SCANS BUT WITHOUT OPPOSITE PHASE ENCODING')\n",
    "subjFolders =[el for el in subjFolders if int(el.split('-')[1])  in subjToAnalyzeWithOnlyAP]\n",
    "subjFolders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get b0 volume indices and perform bval & bvec QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for subjFolder in subjFolders:\n",
    "    scansBaseNames = createSubjectScansBaseNames(subjFolder, data_path)\n",
    "    print('> Verify that data points in the bval files is as expected.')\n",
    "    if pd.read_csv(scansBaseNames['AP_before'] + '.bval', header=None, sep=' ').T.shape[0] != expectedVolums['AP'] or \\\n",
    "        pd.read_csv(scansBaseNames['AP_after'] + '.bval', header=None, sep=' ').T.shape[0] != expectedVolums['AP']:\n",
    "            print(f' *** The number of data points in the bval for one of the scans for subjetc {subjFolder} is not as expected.')\n",
    "            raise Exception(f'The number of data points in the bval for one of the scans for subjetc {subjFolder} is not as expected.')\n",
    "\n",
    "    print('> Verify that data points in the bvec files is as expected.')\n",
    "    if pd.read_csv(scansBaseNames['AP_before'] + '.bvec', header=None, sep=' ').T.shape[0] != expectedVolums['AP'] or \\\n",
    "        pd.read_csv(scansBaseNames['AP_after'] + '.bvec', header=None, sep=' ').T.shape[0] != expectedVolums['AP']:\n",
    "            print(f' *** The number of data points in the bvec for one of the scans for subjetc {subjFolder} is not as expected.')\n",
    "            raise ValueError(f'The number of data points in the bvec for one of the scans for subjetc ' + str(subjFolder) + ' is not as expected.')\n",
    "\n",
    "    print('> Extract B0s:')\n",
    "    B0s_indxs = {}\n",
    "    for scan in scansBaseNames.keys():\n",
    "        B0s=pd.read_csv(scansBaseNames[scan] + '.bval', header=None, sep=' ').T\n",
    "        B0s.columns = ['bval']\n",
    "        B0s_indxs[scan] = list(B0s[B0s.bval < 20].index)\n",
    "\n",
    "    print('> Verify that b0 quantity and indices are as expected.')\n",
    "    if B0s_indxs != expectedB0s_indxs:\n",
    "        print(f' *** The indices of the b0s for one of the scans for subjetc {subjFolder} are not as expected.')\n",
    "        raise ValueError(f'The indices of the b0s for one of the scans for subjetc ' + str(subjFolder) + ' are not as expected.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create pre-processing folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Create preprocessing folders\n"
     ]
    }
   ],
   "source": [
    "print('> Create preprocessing folders')\n",
    "for subjFolder in subjFolders:\n",
    "    try:\n",
    "        os.makedirs(os.path.join(preproc_path, subjFolder), exist_ok=False)\n",
    "        print('>> Created folder: ' + os.path.join(preproc_path, subjFolder))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Preperations of B0s etc (The parllel stuff to run like when preparing before TOPUP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract B0 volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Extract B0s (using the fslroi):\n",
      ">> COMPLETED.\n"
     ]
    }
   ],
   "source": [
    "print('> Extract B0s (using the fslroi):')\n",
    "for subjFolder in subjFolders:\n",
    "    scansBaseNames = createSubjectScansBaseNames(subjFolder, data_path)\n",
    "    for scan in scansBaseNames.keys():\n",
    "        for B0ind in B0s_indxs[scan]:\n",
    "            B0_file_name = os.path.join(preproc_path, subjFolder, subjFolder + '_' + scan + \"_b0_volInd-\" + str(B0ind) + \".nii.gz\")\n",
    "            if not os.path.isfile(f\"{B0_file_name}\"):\n",
    "                print(f'>> runs: fslroi {scansBaseNames[scan]}.nii.gz {B0_file_name} {B0ind} 1')\n",
    "                os.system(f'fslroi {scansBaseNames[scan]}.nii.gz {B0_file_name} {B0ind} 1')\n",
    "print(f\">> COMPLETED.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge B0 volumes for TOPUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Merge B0s for each session (using the fslmerge):\n",
      ">> COMPLETED.\n"
     ]
    }
   ],
   "source": [
    "print('> Merge B0s for each session (using the fslmerge):')\n",
    "#print([scan for scan in sub_B0s_files if 'AP_before' in scan])\n",
    "#print([scan for scan in sub_B0s_files if 'PA_before' in scan])\n",
    "for subjFolder in subjFolders:\n",
    "    # get the sub_B0s_files for the current subject:\n",
    "    scansBaseNames = createSubjectScansBaseNames(subjFolder, data_path)\n",
    "    sub_B0s_files = get_sub_B0_files(subjFolder, scansBaseNames, B0s_indxs)\n",
    "    # merge the B0s for the current subject for each time (before/after):\n",
    "    for time in ['before', 'after']:\n",
    "        output_base_name = f'{os.path.join(preproc_path, subjFolder,subjFolder)}_AP_{time}_b0s'\n",
    "        if not os.path.isfile(f\"{output_base_name}.nii.gz\"):\n",
    "            print(f'>> runs: fslmerge -t {output_base_name}' + ' ' + ' '.join([scan for scan in sub_B0s_files if f'AP_{time}' in scan]))\n",
    "            os.system(f'fslmerge -t {output_base_name}' + ' ' + ' '.join([scan for scan in sub_B0s_files if f'AP_{time}' in scan]))\n",
    "print(f\">> COMPLETED.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the acqparams.txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Create the acqparams.txt files (one per session [before and after])\n",
      ">> COMPLETED.\n"
     ]
    }
   ],
   "source": [
    "print('> Create the acqparams.txt files (one per session [''before'' and ''after''])')\n",
    "\n",
    "for subjFolder in subjFolders:\n",
    "    # get the sub_B0s_files for the current subject:\n",
    "    scansBaseNames = createSubjectScansBaseNames(subjFolder, data_path)\n",
    "    sub_B0s_files = get_sub_B0_files(subjFolder, scansBaseNames, B0s_indxs)\n",
    "\n",
    "    # first get the totalReadoutTime from the json files\n",
    "    total_readout_time = {}\n",
    "    for scanBaseName in scansBaseNames.keys():\n",
    "        with open(scansBaseNames[scanBaseName] + '.json') as json_file:        \n",
    "            scanData = json.load(json_file)\n",
    "            total_readout_time[scanBaseName] = scanData['TotalReadoutTime']\n",
    "\n",
    "    for time in ['before', 'after']:\n",
    "        acqPars=[f'0 -1 0 {total_readout_time[f\"AP_{time}\"]}' for scan in sub_B0s_files if f'AP_{time}' in scan] + [f'0 1 0 {total_readout_time[f\"PA_{time}\"]}' for scan in sub_B0s_files if f'PA_{time}' in scan]\n",
    "        # write a list of strings to a file (one string per line):\n",
    "        with open(os.path.join(preproc_path, subjFolder, subjFolder + f'_{time}_acqparams.txt'), 'w') as f:\n",
    "            for item in acqPars:\n",
    "                f.write(\"%s\\n\" % item)\n",
    "print(f\">> COMPLETED.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) EDDY\n",
    "* A tool for correcting Eddy currents (and motion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average the B0 volums (instead of the TOPUP the .iout file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Average the B0 volums (instead of the TOPUP .iout file). - runs seperately for each time [''before'' and ''after'']\n",
      ">> COMPLETED.\n"
     ]
    }
   ],
   "source": [
    "print(f\">> Average the B0 volums (instead of the TOPUP .iout file). - runs seperately for each time [''before'' and ''after'']\")\n",
    "for subjFolder in subjFolders:\n",
    "      for time in ['before', 'after']:\n",
    "            output_file_name = f\"{os.path.join(preproc_path, subjFolder,subjFolder)}_AP_{time}_b0s_iout_avg.nii.gz\"\n",
    "            if not os.path.isfile(f\"{output_file_name}\"):\n",
    "                  print(f\">> runs: fslmaths {os.path.join(preproc_path, subjFolder, subjFolder)}_AP_{time}_b0s -Tmean {output_file_name}\")\n",
    "                  os.system(f\"fslmaths {os.path.join(preproc_path, subjFolder, subjFolder)}_AP_{time}_b0s -Tmean {output_file_name}\")\n",
    "print(f\">> COMPLETED.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run BET on the averaged B0s volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Extarct the brain form the averaged B0s volume - runs seperately for each time [''before'' and ''after'']\n",
      ">> COMPLETED.\n"
     ]
    }
   ],
   "source": [
    "print(f\">> Extarct the brain form the averaged B0s volume - runs seperately for each time [''before'' and ''after'']\")\n",
    "for subjFolder in subjFolders:\n",
    "      for time in ['before', 'after']:\n",
    "            output_base_name = f\"{os.path.join(preproc_path, subjFolder, subjFolder)}_AP_{time}_b0s_iout_avg_brain\"\n",
    "            if not os.path.isfile(f\"{output_base_name}.nii.gz\") or not os.path.isfile(f\"{output_base_name}_mask.nii.gz\"):\n",
    "                  print(f\">> runs: bet {os.path.join(preproc_path, subjFolder, subjFolder)}_AP_{time}_b0s_iout_avg {output_base_name} -m -f 0.2\")\n",
    "                  os.system(f\"bet {os.path.join(preproc_path, subjFolder, subjFolder)}_AP_{time}_b0s_iout_avg {output_base_name} -m -f 0.2\")\n",
    "print(f\">> COMPLETED.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create index.txt file\n",
    "This file maps the volumes in the main DTI data to the relevant line in the acqparams.txt and in the movpar.txt (assessed movement parameters from TOPUP) files\n",
    "* details in: https://www.youtube.com/watch?v=1T1cRnX7MpA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> create an index.txt file for each subject for each time [before and after] for the EDDY\n",
      ">> COMPLETED.\n"
     ]
    }
   ],
   "source": [
    "print(f\">> create an index.txt file for each subject for each time [before and after] for the EDDY\")\n",
    "for subjFolder in subjFolders:\n",
    "    for time in ['before', 'after']:\n",
    "        ind_to_write = 1\n",
    "        with open(os.path.join(preproc_path, subjFolder, f'{time}_index.txt'), 'w') as f:\n",
    "            for i in range(expectedVolums['AP']):   \n",
    "                if i > 0 and i in expectedB0s_indxs[f'AP_{time}']:\n",
    "                    ind_to_write += 1\n",
    "                f.write(\"%s\\n\" %ind_to_write)\n",
    "print('>> COMPLETED.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run EDDY\n",
    "* Correct for eddy currents and subject movement (and taking to account the suceptibility field calculated by TOPUP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Run EDDY using cheshire server GPU (one per subject per time [before and after])\n",
      "\">> Running this command on cheshire:\n",
      "ssh shirangera@cheshire.tau.ac.il eddy_cuda10.2 --imain=/export2/DATA/HIS/HIS_server/BIDS/sub-204/ses-1/dwi/sub-204_ses-1_acq-ap_run-01_dwi.nii.gz \\\n",
      "                --mask=/export2/DATA/HIS/HIS_server/analysis/dwi_data/preproc/sub-204/sub-204_AP_before_b0s_iout_avg_brain_mask \\\n",
      "                --index=/export2/DATA/HIS/HIS_server/analysis/dwi_data/preproc/sub-204/before_index.txt\\\n",
      "                --acqp=/export2/DATA/HIS/HIS_server/analysis/dwi_data/preproc/sub-204/sub-204_before_acqparams.txt \\\n",
      "                --bvecs=/export2/DATA/HIS/HIS_server/BIDS/sub-204/ses-1/dwi/sub-204_ses-1_acq-ap_run-01_dwi.bvec \\\n",
      "                --bvals=/export2/DATA/HIS/HIS_server/BIDS/sub-204/ses-1/dwi/sub-204_ses-1_acq-ap_run-01_dwi.bval \\\n",
      "                --out=/export2/DATA/HIS/HIS_server/analysis/dwi_data/preproc/sub-204/eddy_unwarped_images_sub-204_before \\\n",
      "                --verbose            \n",
      "\n",
      "\">> Running this command on cheshire:\n",
      "ssh shirangera@cheshire.tau.ac.il eddy_cuda10.2 --imain=/export2/DATA/HIS/HIS_server/BIDS/sub-204/ses-3/dwi/sub-204_ses-3_acq-ap_run-02_dwi.nii.gz \\\n",
      "                --mask=/export2/DATA/HIS/HIS_server/analysis/dwi_data/preproc/sub-204/sub-204_AP_after_b0s_iout_avg_brain_mask \\\n",
      "                --index=/export2/DATA/HIS/HIS_server/analysis/dwi_data/preproc/sub-204/after_index.txt\\\n",
      "                --acqp=/export2/DATA/HIS/HIS_server/analysis/dwi_data/preproc/sub-204/sub-204_after_acqparams.txt \\\n",
      "                --bvecs=/export2/DATA/HIS/HIS_server/BIDS/sub-204/ses-3/dwi/sub-204_ses-3_acq-ap_run-02_dwi.bvec \\\n",
      "                --bvals=/export2/DATA/HIS/HIS_server/BIDS/sub-204/ses-3/dwi/sub-204_ses-3_acq-ap_run-02_dwi.bval \\\n",
      "                --out=/export2/DATA/HIS/HIS_server/analysis/dwi_data/preproc/sub-204/eddy_unwarped_images_sub-204_after \\\n",
      "                --verbose            \n",
      "\n",
      ">> COMPLETED.\n"
     ]
    }
   ],
   "source": [
    "print('> Run EDDY using cheshire server GPU (one per subject per time [''before'' and ''after''])')\n",
    "# files_to_run_each_time = 200\n",
    "# counter=1\n",
    "for subjFolder in subjFolders:\n",
    "    # if int(subjFolder.split('-')[1])<265:\n",
    "    #     continue\n",
    "\n",
    "    # # Stop executing more:\n",
    "    # if counter > files_to_run_each_time:\n",
    "    #     break\n",
    "\n",
    "    scansBaseNames = createSubjectScansBaseNames(subjFolder, data_path)\n",
    "\n",
    "    # check if all expected output files exist:\n",
    "    for time in ['before', 'after']:\n",
    "        \n",
    "        eddy_output_files = glob.glob(os.path.join(preproc_path, subjFolder, f'eddy_unwarped_images_{subjFolder}_{time}*')) # get files in a directory\n",
    "        expected_EDDY_out_files_exist = len(list(set(eddy_output_files))) >= n_expected_EDDY_output_files # check that the number of unique EDDY files is as expected\n",
    "\n",
    "        # Run the launch file if not all output files present:\n",
    "        if not expected_EDDY_out_files_exist:\n",
    "            eddy_command_to_run = f\"{ssh_command_for_cheshire_server} {EDDY_command} --imain={scansBaseNames[f'AP_{time}']}.nii.gz \\\\\\n\\\n",
    "                --mask={os.path.join(preproc_path, subjFolder, subjFolder)}_AP_{time}_b0s_iout_avg_brain_mask \\\\\\n\\\n",
    "                --index={os.path.join(preproc_path, subjFolder, time + '_index.txt')}\\\\\\n\\\n",
    "                --acqp={os.path.join(preproc_path, subjFolder, subjFolder + '_' + time + '_acqparams.txt')} \\\\\\n\\\n",
    "                --bvecs={scansBaseNames[f'AP_{time}']}.bvec \\\\\\n\\\n",
    "                --bvals={scansBaseNames[f'AP_{time}']}.bval \\\\\\n\\\n",
    "                --out={os.path.join(preproc_path, subjFolder,'eddy_unwarped_images_' + subjFolder)}_{time} \\\\\\n\\\n",
    "                --verbose\\\n",
    "            \"\n",
    "            print(f'\">> Running this command on cheshire:\\n{eddy_command_to_run}\\n')\n",
    "            os.system(eddy_command_to_run)\n",
    "            # counter+=1\n",
    "\n",
    "print(f\">> COMPLETED.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run EDDY QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subjFolder in subjFolders:\n",
    "    # get the sub_B0s_files for the current subject:\n",
    "    scansBaseNames = createSubjectScansBaseNames(subjFolder, data_path)\n",
    "\n",
    "    for time in ['before', 'after']:\n",
    "        if not os.path.isdir(os.path.join(preproc_path, subjFolder, f'eddy_unwarped_images_{subjFolder}_{time}.qc')):\n",
    "            print(f\"eddy_quad {os.path.join(preproc_path, subjFolder, f'eddy_unwarped_images_{subjFolder}_{time}')} \\\\\\n\\\n",
    "                -idx {os.path.join(preproc_path, subjFolder, f'{time}_index.txt')} \\\\\\n\\\n",
    "                -par {os.path.join(preproc_path, subjFolder, subjFolder  + f'_{time}_acqparams.txt')} \\\\\\n\\\n",
    "                -m   {os.path.join(preproc_path, subjFolder, f'{subjFolder}_AP_{time}_b0s_iout_avg_brain_mask.nii.gz')} \\\\\\n\\\n",
    "                -b   {scansBaseNames[f'AP_{time}'] + '.bval'}\\\n",
    "                \")\n",
    "            os.system(f\"eddy_quad {os.path.join(preproc_path, subjFolder, f'eddy_unwarped_images_{subjFolder}_{time}')} \\\\\\n\\\n",
    "                -idx {os.path.join(preproc_path, subjFolder, f'{time}_index.txt')} \\\\\\n\\\n",
    "                -par {os.path.join(preproc_path, subjFolder, subjFolder  + f'_{time}_acqparams.txt')} \\\\\\n\\\n",
    "                -m   {os.path.join(preproc_path, subjFolder, f'{subjFolder}_AP_{time}_b0s_iout_avg_brain_mask.nii.gz')} \\\\\\n\\\n",
    "                -b   {scansBaseNames[f'AP_{time}'] + '.bval'}\\\n",
    "                \")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTI modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the DTI folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Create preprocessing folders\n"
     ]
    }
   ],
   "source": [
    "print('> Create preprocessing folders')\n",
    "for subjFolder in subjFolders:\n",
    "    try:\n",
    "        os.makedirs(os.path.join(dti_path, subjFolder), exist_ok=False)\n",
    "        print('>> Created folder: ' + os.path.join(dti_path, subjFolder))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Run DTIFIT (one per subject per time [before and after])\n",
      "dtifit --data=/export2/DATA/HIS/HIS_server/analysis/dwi_data/preproc/sub-110/eddy_unwarped_images_sub-110_before \\\n",
      "                        --mask=/export2/DATA/HIS/HIS_server/analysis/dwi_data/preproc/sub-110/sub-110_AP_before_b0s_iout_avg_brain_mask \\\n",
      "                        --bvecs=/export2/DATA/HIS/HIS_server/BIDS/sub-110/ses-1/dwi/sub-110_ses-1_acq-ap_run-01_dwi.bvec \\\n",
      "                        --bvals=/export2/DATA/HIS/HIS_server/BIDS/sub-110/ses-1/dwi/sub-110_ses-1_acq-ap_run-01_dwi.bval \\\n",
      "                        --out=/export2/DATA/HIS/HIS_server/analysis/dwi_data/dti/sub-110/dti_sub-110_before\n",
      "dtifit --data=/export2/DATA/HIS/HIS_server/analysis/dwi_data/preproc/sub-110/eddy_unwarped_images_sub-110_after \\\n",
      "                        --mask=/export2/DATA/HIS/HIS_server/analysis/dwi_data/preproc/sub-110/sub-110_AP_after_b0s_iout_avg_brain_mask \\\n",
      "                        --bvecs=/export2/DATA/HIS/HIS_server/BIDS/sub-110/ses-1/dwi/sub-110_ses-1_acq-ap_run-02_dwi.bvec \\\n",
      "                        --bvals=/export2/DATA/HIS/HIS_server/BIDS/sub-110/ses-1/dwi/sub-110_ses-1_acq-ap_run-02_dwi.bval \\\n",
      "                        --out=/export2/DATA/HIS/HIS_server/analysis/dwi_data/dti/sub-110/dti_sub-110_after\n",
      "dtifit --data=/export2/DATA/HIS/HIS_server/analysis/dwi_data/preproc/sub-204/eddy_unwarped_images_sub-204_before \\\n",
      "                        --mask=/export2/DATA/HIS/HIS_server/analysis/dwi_data/preproc/sub-204/sub-204_AP_before_b0s_iout_avg_brain_mask \\\n",
      "                        --bvecs=/export2/DATA/HIS/HIS_server/BIDS/sub-204/ses-1/dwi/sub-204_ses-1_acq-ap_run-01_dwi.bvec \\\n",
      "                        --bvals=/export2/DATA/HIS/HIS_server/BIDS/sub-204/ses-1/dwi/sub-204_ses-1_acq-ap_run-01_dwi.bval \\\n",
      "                        --out=/export2/DATA/HIS/HIS_server/analysis/dwi_data/dti/sub-204/dti_sub-204_before\n",
      "dtifit --data=/export2/DATA/HIS/HIS_server/analysis/dwi_data/preproc/sub-204/eddy_unwarped_images_sub-204_after \\\n",
      "                        --mask=/export2/DATA/HIS/HIS_server/analysis/dwi_data/preproc/sub-204/sub-204_AP_after_b0s_iout_avg_brain_mask \\\n",
      "                        --bvecs=/export2/DATA/HIS/HIS_server/BIDS/sub-204/ses-3/dwi/sub-204_ses-3_acq-ap_run-02_dwi.bvec \\\n",
      "                        --bvals=/export2/DATA/HIS/HIS_server/BIDS/sub-204/ses-3/dwi/sub-204_ses-3_acq-ap_run-02_dwi.bval \\\n",
      "                        --out=/export2/DATA/HIS/HIS_server/analysis/dwi_data/dti/sub-204/dti_sub-204_after\n",
      "dtifit --data=/export2/DATA/HIS/HIS_server/analysis/dwi_data/preproc/sub-207/eddy_unwarped_images_sub-207_before \\\n",
      "                        --mask=/export2/DATA/HIS/HIS_server/analysis/dwi_data/preproc/sub-207/sub-207_AP_before_b0s_iout_avg_brain_mask \\\n",
      "                        --bvecs=/export2/DATA/HIS/HIS_server/BIDS/sub-207/ses-1/dwi/sub-207_ses-1_acq-ap_run-01_dwi.bvec \\\n",
      "                        --bvals=/export2/DATA/HIS/HIS_server/BIDS/sub-207/ses-1/dwi/sub-207_ses-1_acq-ap_run-01_dwi.bval \\\n",
      "                        --out=/export2/DATA/HIS/HIS_server/analysis/dwi_data/dti/sub-207/dti_sub-207_before\n",
      "dtifit --data=/export2/DATA/HIS/HIS_server/analysis/dwi_data/preproc/sub-207/eddy_unwarped_images_sub-207_after \\\n",
      "                        --mask=/export2/DATA/HIS/HIS_server/analysis/dwi_data/preproc/sub-207/sub-207_AP_after_b0s_iout_avg_brain_mask \\\n",
      "                        --bvecs=/export2/DATA/HIS/HIS_server/BIDS/sub-207/ses-3/dwi/sub-207_ses-3_acq-ap_run-02_dwi.bvec \\\n",
      "                        --bvals=/export2/DATA/HIS/HIS_server/BIDS/sub-207/ses-3/dwi/sub-207_ses-3_acq-ap_run-02_dwi.bval \\\n",
      "                        --out=/export2/DATA/HIS/HIS_server/analysis/dwi_data/dti/sub-207/dti_sub-207_after\n",
      "dtifit --data=/export2/DATA/HIS/HIS_server/analysis/dwi_data/preproc/sub-209/eddy_unwarped_images_sub-209_before \\\n",
      "                        --mask=/export2/DATA/HIS/HIS_server/analysis/dwi_data/preproc/sub-209/sub-209_AP_before_b0s_iout_avg_brain_mask \\\n",
      "                        --bvecs=/export2/DATA/HIS/HIS_server/BIDS/sub-209/ses-1/dwi/sub-209_ses-1_acq-ap_run-01_dwi.bvec \\\n",
      "                        --bvals=/export2/DATA/HIS/HIS_server/BIDS/sub-209/ses-1/dwi/sub-209_ses-1_acq-ap_run-01_dwi.bval \\\n",
      "                        --out=/export2/DATA/HIS/HIS_server/analysis/dwi_data/dti/sub-209/dti_sub-209_before\n",
      "dtifit --data=/export2/DATA/HIS/HIS_server/analysis/dwi_data/preproc/sub-209/eddy_unwarped_images_sub-209_after \\\n",
      "                        --mask=/export2/DATA/HIS/HIS_server/analysis/dwi_data/preproc/sub-209/sub-209_AP_after_b0s_iout_avg_brain_mask \\\n",
      "                        --bvecs=/export2/DATA/HIS/HIS_server/BIDS/sub-209/ses-3/dwi/sub-209_ses-3_acq-ap_run-02_dwi.bvec \\\n",
      "                        --bvals=/export2/DATA/HIS/HIS_server/BIDS/sub-209/ses-3/dwi/sub-209_ses-3_acq-ap_run-02_dwi.bval \\\n",
      "                        --out=/export2/DATA/HIS/HIS_server/analysis/dwi_data/dti/sub-209/dti_sub-209_after\n",
      "dtifit --data=/export2/DATA/HIS/HIS_server/analysis/dwi_data/preproc/sub-255/eddy_unwarped_images_sub-255_before \\\n",
      "                        --mask=/export2/DATA/HIS/HIS_server/analysis/dwi_data/preproc/sub-255/sub-255_AP_before_b0s_iout_avg_brain_mask \\\n",
      "                        --bvecs=/export2/DATA/HIS/HIS_server/BIDS/sub-255/ses-1/dwi/sub-255_ses-1_acq-ap_run-01_dwi.bvec \\\n",
      "                        --bvals=/export2/DATA/HIS/HIS_server/BIDS/sub-255/ses-1/dwi/sub-255_ses-1_acq-ap_run-01_dwi.bval \\\n",
      "                        --out=/export2/DATA/HIS/HIS_server/analysis/dwi_data/dti/sub-255/dti_sub-255_before\n",
      "dtifit --data=/export2/DATA/HIS/HIS_server/analysis/dwi_data/preproc/sub-255/eddy_unwarped_images_sub-255_after \\\n",
      "                        --mask=/export2/DATA/HIS/HIS_server/analysis/dwi_data/preproc/sub-255/sub-255_AP_after_b0s_iout_avg_brain_mask \\\n",
      "                        --bvecs=/export2/DATA/HIS/HIS_server/BIDS/sub-255/ses-3/dwi/sub-255_ses-3_acq-ap_run-02_dwi.bvec \\\n",
      "                        --bvals=/export2/DATA/HIS/HIS_server/BIDS/sub-255/ses-3/dwi/sub-255_ses-3_acq-ap_run-02_dwi.bval \\\n",
      "                        --out=/export2/DATA/HIS/HIS_server/analysis/dwi_data/dti/sub-255/dti_sub-255_after\n"
     ]
    }
   ],
   "source": [
    "print('> Run DTIFIT (one per subject per time [''before'' and ''after''])')\n",
    "# files_to_run_each_time = 200\n",
    "# counter=1\n",
    "for subjFolder in subjFolders:\n",
    "\n",
    "    scansBaseNames = createSubjectScansBaseNames(subjFolder, data_path)\n",
    "\n",
    "    # check if all expected output files exist:\n",
    "    for time in ['before', 'after']:\n",
    "        \n",
    "        dtifit_output_files = glob.glob(os.path.join(dti_path, subjFolder, f'dti_{subjFolder}_{time}*')) # get files in a directory\n",
    "        expected_DTIFIT_out_files_exist = len(list(set(dtifit_output_files))) >= n_expected_DTIFIT_output_files # check that the number of unique EDDY files is as expected\n",
    "\n",
    "        # Run the launch file if not all output files present:\n",
    "        if not expected_DTIFIT_out_files_exist:\n",
    "            dtifit_command = f\"dtifit --data={os.path.join(preproc_path, subjFolder, f'eddy_unwarped_images_{subjFolder}_{time}')} \\\\\\n\\\n",
    "                        --mask={os.path.join(preproc_path, subjFolder, f'{subjFolder}_AP_{time}_b0s_iout_avg_brain_mask')} \\\\\\n\\\n",
    "                        --bvecs={scansBaseNames[f'AP_{time}'] + '.bvec'} \\\\\\n\\\n",
    "                        --bvals={scansBaseNames[f'AP_{time}'] + '.bval'} \\\\\\n\\\n",
    "                        --out={os.path.join(dti_path, subjFolder, f'dti_{subjFolder}_{time}')}\"\n",
    "\n",
    "            print(dtifit_command)\n",
    "            os.system(dtifit_command)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
