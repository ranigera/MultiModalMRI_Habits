{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze DTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "# -----------------------------------------------------------------------------\n",
    "# import pip\n",
    "# pip.main(['install', 'seaborn']) \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "import sys\n",
    "import json\n",
    "import platform\n",
    "import socket\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "# -----------------------------------------------------------------------------\n",
    "MANUALLY_EXCLUDE_SUBJECTS = []\n",
    "\n",
    "running_on = 'server' if 'Linux' in platform.system() else 'my_mac'\n",
    "\n",
    "if running_on == 'my_mac':\n",
    "    data_path = '/Users/ranigera/Dropbox/DTI_tests'\n",
    "    preproc_path = '/Users/ranigera/Dropbox/DTI_tests/preproc'\n",
    "    dti_path = '/Users/ranigera/Dropbox/DTI_tests/dti'\n",
    "    stats_path = '/Users/ranigera/Dropbox/DTI_tests/stats_alt_reg'\n",
    "    models_path = stats_path + '/models'\n",
    "    launch_files_path = '/Users/ranigera/Dropbox/DTI_analysis/launch_files'\n",
    "else:\n",
    "    data_path = '/export2/DATA/HIS/HIS_server/BIDS'\n",
    "    preproc_path = '/export2/DATA/HIS/HIS_server/analysis/dwi_data/preproc'\n",
    "    dti_path = '/export2/DATA/HIS/HIS_server/analysis/dwi_data/dti'\n",
    "    stats_path = '/export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg'\n",
    "    models_path = stats_path + '/models'\n",
    "    launch_files_path = '/export2/DATA/HIS/HIS_server/codes_dwi/launch_files'\n",
    "    behav_data_path = '/export2/DATA/HIS/HIS_server/analysis/behavior_analysis_output/my_databases/txt_data'\n",
    "\n",
    "fmriPrepAnatomyDerivatives_path = data_path + '/derivatives/fmriprep'\n",
    "\n",
    "expectedVolums = {\n",
    "    'AP': 69,\n",
    "    'PA' : 7,\n",
    "    }\n",
    "expectedB0s_indxs = {\n",
    "    'AP_before': [0, 1, 18, 35, 52],\n",
    "    'PA_before': [0, 2, 3, 4, 5, 6],\n",
    "    'AP_after': [0, 1, 18, 35, 52],\n",
    "    'PA_after': [0, 2, 3, 4, 5, 6]\n",
    "    }\n",
    "\n",
    "n_cores_TOPUP = 2\n",
    "\n",
    "# setting EDDY stuff:\n",
    "EDDY_command = 'eddy_openmp' if running_on == 'server' else 'eddy' # for the boost server\n",
    "EDDY_command = 'eddy_cuda10.2' if running_on == 'server' else 'eddy' # for the cheshire server\n",
    "ssh_command_for_cheshire_server = 'ssh shirangera@cheshire.tau.ac.il' if 'boost' in socket.gethostname() else ''\n",
    "\n",
    "n_expected_EDDY_output_files = 13\n",
    "n_cores_EDDY = 4 # relevant only for running using files (currently disabled as I run it on cheshire's GPU)\n",
    "\n",
    "n_expected_DTIFIT_output_files = 10\n",
    "\n",
    "masks_paths = {\n",
    "    'Caudate': '/export2/DATA/HIS/HIS_server/DTI_assitance_files/masks/Harvard-Oxford/CaudateHead_Y-larger-than-1-mask.nii.gz',\n",
    "    'Putamen': '/export2/DATA/HIS/HIS_server/DTI_assitance_files/masks/Harvard-Oxford/Putamen-mask.nii.gz',\n",
    "    'vmPFC':   '/export2/DATA/HIS/HIS_server/DTI_assitance_files/masks/Harvard-Oxford/vmPFC-mask.nii.gz',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added in this file to adjust some paths:\n",
    "stats_path_NOTOPUP_EXCLUDED = stats_path + '_NOTOPUP_EXCLUDED'\n",
    "models_path = os.path.join(stats_path_NOTOPUP_EXCLUDED, 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "# -----------------------------------------------------------------------------\n",
    "def createSubjectScansBaseNames(subjFolder, data_path):\n",
    "    sub = int(subjFolder.split(\"-\",1)[1])\n",
    "    group = '1day' if sub < 200 else '3day'\n",
    "    last_sess = group[0]\n",
    "    DWI_path_before = os.path.join(data_path, subjFolder, 'ses-1/dwi/')\n",
    "    DWI_path_after = os.path.join(data_path, subjFolder, f'ses-{last_sess}/dwi/')\n",
    "    scansBaseNames = {\n",
    "        'AP_before': f'{os.path.join(DWI_path_before, \"sub-\" + str(sub) + \"_ses-1_acq-ap_run-01_dwi\")}',\n",
    "        'PA_before' : f'{os.path.join(DWI_path_before, \"sub-\" + str(sub) + \"_ses-1_acq-pa_run-01_dwi\")}',\n",
    "        'AP_after' : f'{os.path.join(DWI_path_after, \"sub-\" + str(sub) + \"_ses-\" + last_sess + \"_acq-ap_run-02_dwi\")}',\n",
    "        'PA_after' : f'{os.path.join(DWI_path_after, \"sub-\" + str(sub) + \"_ses-\" + last_sess + \"_acq-pa_run-02_dwi\")}'\n",
    "        }\n",
    "    return scansBaseNames\n",
    "\n",
    "def get_sub_B0_files(subjFolder, scansBaseNames, B0s_indxs):\n",
    "    sub_B0s_files = []\n",
    "    for scan in scansBaseNames.keys():\n",
    "        for B0ind in B0s_indxs[scan]:\n",
    "            B0_file_name = os.path.join(preproc_path, subjFolder, subjFolder + '_' + scan + \"_b0_volInd-\" + str(B0ind) + \".nii.gz\")\n",
    "            sub_B0s_files.append(B0_file_name)\n",
    "    return sub_B0s_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Get sub folders\n"
     ]
    }
   ],
   "source": [
    "# Get folders and remove excluded subjects\n",
    "# -----------------------------------------------------------------------------\n",
    "print('>> Get sub folders')\n",
    "subjFolders = [el for el in os.listdir(data_path) if 'sub' in el]\n",
    "\n",
    "if running_on == 'my_mac':\n",
    "    print('>> Get exclusion list')\n",
    "    with open('/Users/ranigera/Google_Drive_TAU/Experiments/HIS_STUDY/Analysis/codes/paths_and_vars.py') as txtFile:\n",
    "        txt = txtFile.read()\n",
    "    participantsToExclude = [int(el) for el in txt.split('participantsToExclude = [')[1].split(']')[0].replace('\\n','').replace('\\n','').replace(\"'\",\"\").split(',')]\n",
    "\n",
    "    print('>> Remove sub folders of excluded participants in case they are there')\n",
    "    subjFolders = [el for el in subjFolders if int(el.split('-')[1]) not in participantsToExclude]\n",
    "\n",
    "if MANUALLY_EXCLUDE_SUBJECTS:\n",
    "    subjFolders = [el for el in subjFolders if int(el.split('-')[1]) not in MANUALLY_EXCLUDE_SUBJECTS]\n",
    "    \n",
    "subjFolders.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for missing scans or wrong phase encoding directions for ALL SUBJECTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Verify that all the scans exist and that the phase encoding directions are as they should.\n",
      " *** There is a problem with the scanning directions: /export2/DATA/HIS/HIS_server/BIDS/sub-204/ses-3/dwi/sub-204_ses-3_acq-pa_run-02_dwi.json is defined as j-.\n",
      " *** There is a problem with the scanning directions: /export2/DATA/HIS/HIS_server/BIDS/sub-207/ses-1/dwi/sub-207_ses-1_acq-pa_run-01_dwi.json is defined as j-.\n",
      " *** There is a problem with the scanning directions: /export2/DATA/HIS/HIS_server/BIDS/sub-209/ses-1/dwi/sub-209_ses-1_acq-pa_run-01_dwi.json is defined as j-.\n",
      " *** Scan not found: /export2/DATA/HIS/HIS_server/BIDS/sub-255/ses-1/dwi/sub-255_ses-1_acq-pa_run-01_dwi.json\n",
      " *** Scan not found: /export2/DATA/HIS/HIS_server/BIDS/sub-259/ses-3/dwi/sub-259_ses-3_acq-ap_run-02_dwi.json\n",
      " *** Scan not found: /export2/DATA/HIS/HIS_server/BIDS/sub-259/ses-3/dwi/sub-259_ses-3_acq-pa_run-02_dwi.json\n"
     ]
    }
   ],
   "source": [
    "print ('>> Verify that all the scans exist and that the phase encoding directions are as they should.')\n",
    "subjectsWithAProblem = []\n",
    "for subjFolder in subjFolders:\n",
    "    sub = int(subjFolder.split(\"-\",1)[1])\n",
    "    scansBaseNames = createSubjectScansBaseNames(subjFolder, data_path)\n",
    "    for scan in scansBaseNames.keys():\n",
    "        # print(scansBaseNames[scan] + '.json')\n",
    "        # print(scanData['PhaseEncodingDirection'])\n",
    "        if not os.path.exists(scansBaseNames[scan] + '.json'):\n",
    "            subjectsWithAProblem.append(sub)\n",
    "            print(' *** Scan not found: ' + scansBaseNames[scan] + '.json')\n",
    "            continue\n",
    "        with open(scansBaseNames[scan] + '.json') as json_file:        \n",
    "            scanData = json.load(json_file)\n",
    "            if ('acq-ap_' in scansBaseNames[scan] and scanData['PhaseEncodingDirection'] != 'j-') or \\\n",
    "                ('acq-pa_' in scansBaseNames[scan] and scanData['PhaseEncodingDirection'] != 'j'):\n",
    "                subjectsWithAProblem.append(sub)\n",
    "                print(' *** There is a problem with the scanning directions: ' + scansBaseNames[scan] + '.json is defined as ' + scanData['PhaseEncodingDirection'] + '.')\n",
    "                continue\n",
    "\n",
    "subjectsWithAProblem = list(set(subjectsWithAProblem))\n",
    "subjectsWithAProblem.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Remove subjects with a problem and sort the list\n"
     ]
    }
   ],
   "source": [
    "# Remove subjects with a problem\n",
    "# -----------------------------------------------------------------------------\n",
    "print('>> Remove subjects with a problem and sort the list')\n",
    "subjFolders =[el for el in subjFolders if int(el.split('-')[1]) not in subjectsWithAProblem]\n",
    "subjFolders.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get b0 volume indices and perform bval & bvec QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n",
      "> Verify that data points in the bval files is as expected.\n",
      "> Verify that data points in the bvec files is as expected.\n",
      "> Extract B0s:\n",
      "> Verify that b0 quantity and indices are as expected.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for subjFolder in subjFolders:\n",
    "    scansBaseNames = createSubjectScansBaseNames(subjFolder, data_path)\n",
    "    print('> Verify that data points in the bval files is as expected.')\n",
    "    if pd.read_csv(scansBaseNames['AP_before'] + '.bval', header=None, sep=' ').T.shape[0] != expectedVolums['AP'] or \\\n",
    "        pd.read_csv(scansBaseNames['PA_before'] + '.bval', header=None, sep=' ').T.shape[0] != expectedVolums['PA'] or \\\n",
    "        pd.read_csv(scansBaseNames['AP_after'] + '.bval', header=None, sep=' ').T.shape[0] != expectedVolums['AP'] or \\\n",
    "        pd.read_csv(scansBaseNames['PA_after'] + '.bval', header=None, sep=' ').T.shape[0] != expectedVolums['PA']:\n",
    "            print(f' *** The number of data points in the bval for one of the scans for subjetc {sub} is not as expected.')\n",
    "            raise Exception(f'The number of data points in the bval for one of the scans for subjetc {sub} is not as expected.')\n",
    "\n",
    "    print('> Verify that data points in the bvec files is as expected.')\n",
    "    if pd.read_csv(scansBaseNames['AP_before'] + '.bvec', header=None, sep=' ').T.shape[0] != expectedVolums['AP'] or \\\n",
    "        pd.read_csv(scansBaseNames['PA_before'] + '.bvec', header=None, sep=' ').T.shape[0] != expectedVolums['PA'] or \\\n",
    "        pd.read_csv(scansBaseNames['AP_after'] + '.bvec', header=None, sep=' ').T.shape[0] != expectedVolums['AP'] or \\\n",
    "        pd.read_csv(scansBaseNames['PA_after'] + '.bvec', header=None, sep=' ').T.shape[0] != expectedVolums['PA']:\n",
    "            print(f' *** The number of data points in the bvec for one of the scans for subjetc {sub} is not as expected.')\n",
    "            raise ValueError(f'The number of data points in the bvec for one of the scans for subjetc ' + str(sub) + ' is not as expected.')\n",
    "\n",
    "    print('> Extract B0s:')\n",
    "    B0s_indxs = {}\n",
    "    for scan in scansBaseNames.keys():\n",
    "        B0s=pd.read_csv(scansBaseNames[scan] + '.bval', header=None, sep=' ').T\n",
    "        B0s.columns = ['bval']\n",
    "        B0s_indxs[scan] = list(B0s[B0s.bval < 20].index)\n",
    "\n",
    "    print('> Verify that b0 quantity and indices are as expected.')\n",
    "    if B0s_indxs != expectedB0s_indxs:\n",
    "        print(f' *** The indices of the b0s for one of the scans for subjetc {sub} are not as expected.')\n",
    "        raise ValueError(f'The indices of the b0s for one of the scans for subjetc ' + str(sub) + ' are not as expected.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather subject folders again based on the dti folders!\n",
    "Make sure that before that you ran DWI_pipeline_noTOPUPpipeline.ipynb to include subject without TOPUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Get sub folders from the DTI map folder\n"
     ]
    }
   ],
   "source": [
    "print('>> Get sub folders from the DTI map folder')\n",
    "subjFoldersDTI = [el for el in os.listdir(dti_path) if 'sub' in el]\n",
    "subjFoldersDTI.sort()\n",
    "\n",
    "subjToAnalyzeWithOnlyAP = [110, 204, 207, 209, 255] # add here 159 when wish to repeat analyses without this participant (relevant for the vMPFC MD in the short training)\n",
    "subjFoldersDTI = [sub for sub in subjFoldersDTI if int(sub.split('-')[1]) not in subjToAnalyzeWithOnlyAP]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subjFoldersDTI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis: BETWEEN-GRUOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis is based on this page: https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/GLM#ANOVA:_2-groups.2C_2-levels_per_subject_.282-way_Mixed_Effect_ANOVA.29\n",
    "Specifically:\n",
    "\"Randomise details\n",
    "Due to how the data would need to be permuted, the FEAT model may not be used in randomise. Instead, just as in the paired t-test example, paired differences within-subject would be computed via fslmaths and a two-sample t-test could be used to test whether the run1-run2 difference differed between the two groups.\"\n",
    "\n",
    "Thus the first step (calculating the differences within-subject) is as here: https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/GLM#Single-Group_Paired_Difference_.28Paired_T-Test.29\n",
    "And the second is as here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> UNIQUE PART FOR THE NONTOPUP EXCLUDED <<<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('> Create one file with all subjects of AFTER minus BEFORE and one withh all Before minus AFTER for the MD and FA maps')\n",
    "for map_type in ['MD','FA']:          \n",
    "    print(f\"fslmerge -t {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f'NOTOPUP_EXCLUDED_ALL_SUBJECTS_AFTER_minus_BEFORE_{map_type}')} {' '.join([os.path.join(stats_path, map_type, file) for file in [f'{sub}_in-MNI_{map_type}_AFTER-minus-BEFORE.nii.gz' for sub in subjFoldersDTI]])}\")\n",
    "    os.system(f\"fslmerge -t {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f'NOTOPUP_EXCLUDED_ALL_SUBJECTS_AFTER_minus_BEFORE_{map_type}')} {' '.join([os.path.join(stats_path, map_type, file) for file in [f'{sub}_in-MNI_{map_type}_AFTER-minus-BEFORE.nii.gz' for sub in subjFoldersDTI]])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Create a merged file for each group\n"
     ]
    }
   ],
   "source": [
    "print('> Create a merged file for each group')\n",
    "N_1_day_group = len(set([sub for sub in subjFoldersDTI if int(sub.split('-')[-1])<200]))\n",
    "N_3_day_group = len(set([sub for sub in subjFoldersDTI if int(sub.split('-')[-1])>200]))\n",
    "\n",
    "for map_type in ['MD','FA']:          \n",
    "    os.system(f\"fslroi {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f'NOTOPUP_EXCLUDED_ALL_SUBJECTS_AFTER_minus_BEFORE_{map_type}')} {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f'NOTOPUP_EXCLUDED_{map_type}_short')} 0 {N_1_day_group}\")\n",
    "    os.system(f\"fslroi {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f'NOTOPUP_EXCLUDED_ALL_SUBJECTS_AFTER_minus_BEFORE_{map_type}')} {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f'NOTOPUP_EXCLUDED_{map_type}_long')} {N_1_day_group} {N_3_day_group}\")\n",
    "    #os.system(f\"fslmerge -t {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f'NOTOPUP_EXCLUDED_ALL_SUBJECTS_AFTER_minus_BEFORE_{map_type}')} {' '.join([os.path.join(stats_path, map_type, file) for file in [f'{sub}_in-MNI_{map_type}_AFTER-minus-BEFORE.nii.gz' for sub in subjFoldersDTI]])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Average\n"
     ]
    }
   ],
   "source": [
    "print('> Average')\n",
    "for map_type in ['MD','FA']:          \n",
    "    os.system(f\"fslmaths {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f'NOTOPUP_EXCLUDED_{map_type}_short')} -Tmean {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f'NOTOPUP_EXCLUDED_{map_type}_short_avg')}\")\n",
    "    os.system(f\"fslmaths {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f'NOTOPUP_EXCLUDED_{map_type}_long')} -Tmean {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f'NOTOPUP_EXCLUDED_{map_type}_long_avg')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Create the between group design matrix (of the calculated differences) - same is unpaired t-test (on the after-before differences)\n",
      "design_ttest2 /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/between_groups/design_unpaired_ttest 60 56\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('> Create the between group design matrix (of the calculated differences) - same is unpaired t-test (on the after-before differences)')\n",
    "# create the folder\n",
    "try:\n",
    "    os.makedirs(os.path.join(models_path, 'between_groups'), exist_ok=False)\n",
    "    print('>> Created folder: ' + os.path.join(models_path, 'between_groups'))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "#create the design matrix and contrasts:\n",
    "N_1_day_group = len(set([sub for sub in subjFoldersDTI if int(sub.split('-')[-1])<200]))\n",
    "N_3_day_group = len(set([sub for sub in subjFoldersDTI if int(sub.split('-')[-1])>200]))\n",
    "\n",
    "print(f\"design_ttest2 {os.path.join(models_path, 'between_groups','design_unpaired_ttest')} {N_1_day_group} {N_3_day_group}\")\n",
    "os.system(f\"design_ttest2 {os.path.join(models_path, 'between_groups','design_unpaired_ttest')} {N_1_day_group} {N_3_day_group}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to run randomise for the between-group analyses:\n",
    "def runRandomiseGroupAnalysis(map_type, region):\n",
    "    print(f'Running randomise for between-groups analysis:\\n--- {map_type} | {region} ---\\n')\n",
    "    #print(f'MAKE SURE TO REPLACE IN THE FUNCTION THE the print to os.system before running the command !!!\\n')\n",
    "    os.system(f\"randomise_parallel \\\n",
    "        -i {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f'NOTOPUP_EXCLUDED_ALL_SUBJECTS_AFTER_minus_BEFORE_{map_type}')} \\\n",
    "        -o {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f'NOTOPUP_EXCLUDED_ALL_SUBJECTS_AFTER_minus_BEFORE_{map_type}_{region}')} \\\n",
    "        {'' if region=='whole_brain' else f'-m {masks_paths[region]}'} \\\n",
    "        -d {os.path.join(stats_path_NOTOPUP_EXCLUDED, 'models', 'between_groups', f'design_unpaired_ttest.mat')} \\\n",
    "        -t {os.path.join(stats_path_NOTOPUP_EXCLUDED, 'models', 'between_groups', f'design_unpaired_ttest.con')}  \\\n",
    "        -n 6000 -C 3.1\")\n",
    "\n",
    "def runRandomiseGroupAnalysis_ALL_TERMINAL(map_types, regions):\n",
    "    commandList = []\n",
    "    for map_type in map_types:\n",
    "        for region in regions:\n",
    "            print(f'Running randomise for between-groups analysis:\\n--- {map_type} | {region} ---\\n')\n",
    "            #print(f'MAKE SURE TO REPLACE IN THE FUNCTION THE the print to os.system before running the command !!!\\n')\n",
    "            commandList.append(f\"randomise_parallel \\\n",
    "                -i {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f'NOTOPUP_EXCLUDED_ALL_SUBJECTS_AFTER_minus_BEFORE_{map_type}')} \\\n",
    "                -o {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f'NOTOPUP_EXCLUDED_ALL_SUBJECTS_AFTER_minus_BEFORE_{map_type}_{region}')} \\\n",
    "                {'' if region=='whole_brain' else f'-m {masks_paths[region]}'} \\\n",
    "                -d {os.path.join(stats_path_NOTOPUP_EXCLUDED, 'models', 'between_groups', f'design_unpaired_ttest.mat')} \\\n",
    "                -t {os.path.join(stats_path_NOTOPUP_EXCLUDED, 'models', 'between_groups', f'design_unpaired_ttest.con')}  \\\n",
    "                -n 6000 -C 3.1 && \")\n",
    "    commandList[-1] = commandList[-1][:-3]  \n",
    "    return ''.join(commandList)\n",
    "\n",
    "# Get cluster info:\n",
    "def getClusterInfoGroupAnalysis(map_type, region, ReversedSigThresh='0.90'):\n",
    "    print(f'Testing stats for between-group analysis:\\n--- {map_type} | {region} ---\\n')\n",
    "    print(f'Test 1-Day > 3-Day\\n')\n",
    "    # * Increase in FA in the 1-day relatvie to the 3-days (or a decrease in the 3-day...)\n",
    "    print(os.popen(f'cluster \\\n",
    "        -i {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f\"NOTOPUP_EXCLUDED_ALL_SUBJECTS_AFTER_minus_BEFORE_{map_type}_{region}_clusterm_corrp_tstat1.nii.gz\")} \\\n",
    "        -c {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f\"NOTOPUP_EXCLUDED_ALL_SUBJECTS_AFTER_minus_BEFORE_{map_type}_{region}_tstat1.nii.gz\")}\\\n",
    "        -t {ReversedSigThresh} \\\n",
    "        --scalarname=corrp' + '\"1-p\"').read())\n",
    "\n",
    "    print(f'3-Day > 1-Day\\n')\n",
    "    # * Increase in FA in the 3-day relatvie to the 1-days (or a decrease in the 1-day...)\n",
    "    print(os.popen(f'cluster \\\n",
    "        -i {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f\"NOTOPUP_EXCLUDED_ALL_SUBJECTS_AFTER_minus_BEFORE_{map_type}_{region}_clusterm_corrp_tstat2.nii.gz\")} \\\n",
    "        -c {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f\"NOTOPUP_EXCLUDED_ALL_SUBJECTS_AFTER_minus_BEFORE_{map_type}_{region}_tstat2.nii.gz\")}\\\n",
    "        -t {ReversedSigThresh} \\\n",
    "        --scalarname=corrp' + '\"1-p\"').read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run in terminal and summarize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running randomise for between-groups analysis:\n",
      "--- FA | whole_brain ---\n",
      "\n",
      "Running randomise for between-groups analysis:\n",
      "--- FA | Putamen ---\n",
      "\n",
      "Running randomise for between-groups analysis:\n",
      "--- FA | Caudate ---\n",
      "\n",
      "Running randomise for between-groups analysis:\n",
      "--- FA | vmPFC ---\n",
      "\n",
      "Running randomise for between-groups analysis:\n",
      "--- MD | whole_brain ---\n",
      "\n",
      "Running randomise for between-groups analysis:\n",
      "--- MD | Putamen ---\n",
      "\n",
      "Running randomise for between-groups analysis:\n",
      "--- MD | Caudate ---\n",
      "\n",
      "Running randomise for between-groups analysis:\n",
      "--- MD | vmPFC ---\n",
      "\n",
      "randomise_parallel                 -i /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/FA/NOTOPUP_EXCLUDED_ALL_SUBJECTS_AFTER_minus_BEFORE_FA                 -o /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/FA/NOTOPUP_EXCLUDED_ALL_SUBJECTS_AFTER_minus_BEFORE_FA_whole_brain                                  -d /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/between_groups/design_unpaired_ttest.mat                 -t /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/between_groups/design_unpaired_ttest.con                  -n 6000 -C 3.1 && randomise_parallel                 -i /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/FA/NOTOPUP_EXCLUDED_ALL_SUBJECTS_AFTER_minus_BEFORE_FA                 -o /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/FA/NOTOPUP_EXCLUDED_ALL_SUBJECTS_AFTER_minus_BEFORE_FA_Putamen                 -m /export2/DATA/HIS/HIS_server/DTI_assitance_files/masks/Harvard-Oxford/Putamen-mask.nii.gz                 -d /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/between_groups/design_unpaired_ttest.mat                 -t /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/between_groups/design_unpaired_ttest.con                  -n 6000 -C 3.1 && randomise_parallel                 -i /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/FA/NOTOPUP_EXCLUDED_ALL_SUBJECTS_AFTER_minus_BEFORE_FA                 -o /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/FA/NOTOPUP_EXCLUDED_ALL_SUBJECTS_AFTER_minus_BEFORE_FA_Caudate                 -m /export2/DATA/HIS/HIS_server/DTI_assitance_files/masks/Harvard-Oxford/CaudateHead_Y-larger-than-1-mask.nii.gz                 -d /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/between_groups/design_unpaired_ttest.mat                 -t /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/between_groups/design_unpaired_ttest.con                  -n 6000 -C 3.1 && randomise_parallel                 -i /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/FA/NOTOPUP_EXCLUDED_ALL_SUBJECTS_AFTER_minus_BEFORE_FA                 -o /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/FA/NOTOPUP_EXCLUDED_ALL_SUBJECTS_AFTER_minus_BEFORE_FA_vmPFC                 -m /export2/DATA/HIS/HIS_server/DTI_assitance_files/masks/Harvard-Oxford/vmPFC-mask.nii.gz                 -d /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/between_groups/design_unpaired_ttest.mat                 -t /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/between_groups/design_unpaired_ttest.con                  -n 6000 -C 3.1 && randomise_parallel                 -i /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/MD/NOTOPUP_EXCLUDED_ALL_SUBJECTS_AFTER_minus_BEFORE_MD                 -o /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/MD/NOTOPUP_EXCLUDED_ALL_SUBJECTS_AFTER_minus_BEFORE_MD_whole_brain                                  -d /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/between_groups/design_unpaired_ttest.mat                 -t /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/between_groups/design_unpaired_ttest.con                  -n 6000 -C 3.1 && randomise_parallel                 -i /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/MD/NOTOPUP_EXCLUDED_ALL_SUBJECTS_AFTER_minus_BEFORE_MD                 -o /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/MD/NOTOPUP_EXCLUDED_ALL_SUBJECTS_AFTER_minus_BEFORE_MD_Putamen                 -m /export2/DATA/HIS/HIS_server/DTI_assitance_files/masks/Harvard-Oxford/Putamen-mask.nii.gz                 -d /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/between_groups/design_unpaired_ttest.mat                 -t /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/between_groups/design_unpaired_ttest.con                  -n 6000 -C 3.1 && randomise_parallel                 -i /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/MD/NOTOPUP_EXCLUDED_ALL_SUBJECTS_AFTER_minus_BEFORE_MD                 -o /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/MD/NOTOPUP_EXCLUDED_ALL_SUBJECTS_AFTER_minus_BEFORE_MD_Caudate                 -m /export2/DATA/HIS/HIS_server/DTI_assitance_files/masks/Harvard-Oxford/CaudateHead_Y-larger-than-1-mask.nii.gz                 -d /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/between_groups/design_unpaired_ttest.mat                 -t /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/between_groups/design_unpaired_ttest.con                  -n 6000 -C 3.1 && randomise_parallel                 -i /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/MD/NOTOPUP_EXCLUDED_ALL_SUBJECTS_AFTER_minus_BEFORE_MD                 -o /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/MD/NOTOPUP_EXCLUDED_ALL_SUBJECTS_AFTER_minus_BEFORE_MD_vmPFC                 -m /export2/DATA/HIS/HIS_server/DTI_assitance_files/masks/Harvard-Oxford/vmPFC-mask.nii.gz                 -d /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/between_groups/design_unpaired_ttest.mat                 -t /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/between_groups/design_unpaired_ttest.con                  -n 6000 -C 3.1 \n"
     ]
    }
   ],
   "source": [
    "# copy the command to the terminal to run all the between group randomise analyses:\n",
    "command=runRandomiseGroupAnalysis_ALL_TERMINAL(map_types=['FA', 'MD'], regions=['whole_brain', 'Putamen', 'Caudate', 'vmPFC'])\n",
    "print(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing stats for between-group analysis:\n",
      "--- FA | whole_brain ---\n",
      "\n",
      "Test 1-Day > 3-Day\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "3-Day > 1-Day\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Testing stats for between-group analysis:\n",
      "--- FA | Putamen ---\n",
      "\n",
      "Test 1-Day > 3-Day\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "3-Day > 1-Day\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Testing stats for between-group analysis:\n",
      "--- FA | Caudate ---\n",
      "\n",
      "Test 1-Day > 3-Day\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "3-Day > 1-Day\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Testing stats for between-group analysis:\n",
      "--- FA | vmPFC ---\n",
      "\n",
      "Test 1-Day > 3-Day\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "3-Day > 1-Day\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Testing stats for between-group analysis:\n",
      "--- MD | whole_brain ---\n",
      "\n",
      "Test 1-Day > 3-Day\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "3-Day > 1-Day\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Testing stats for between-group analysis:\n",
      "--- MD | Putamen ---\n",
      "\n",
      "Test 1-Day > 3-Day\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "3-Day > 1-Day\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Testing stats for between-group analysis:\n",
      "--- MD | Caudate ---\n",
      "\n",
      "Test 1-Day > 3-Day\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "3-Day > 1-Day\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "2\t8\t0.944\t43\t64\t37\t42.5\t65.4\t37.4\t3.32\t43\t65\t37\t3.24\n",
      "1\t8\t0.945\t42\t68\t40\t42.5\t69\t40.2\t3.43\t42\t69\t40\t3.26\n",
      "\n",
      "Testing stats for between-group analysis:\n",
      "--- MD | vmPFC ---\n",
      "\n",
      "Test 1-Day > 3-Day\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "3-Day > 1-Day\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "1\t121\t0.994\t45\t80\t27\t44.9\t79.5\t31.3\t4.22\t47\t79\t32\t3.44\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the cluster info for the between-group analyses:\n",
    "for map_type in ['FA', 'MD']:\n",
    "    for region in ['whole_brain', 'Putamen', 'Caudate', 'vmPFC']:\n",
    "        getClusterInfoGroupAnalysis(map_type, region, ReversedSigThresh='0.90')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics - individual differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the habit index csv file (by running a dedicated matlab folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATLAB is selecting SOFTWARE OPENGL rendering.\n",
      "\u001b[?1h\u001b=\n",
      "                            < M A T L A B (R) >\n",
      "                  Copyright 1984-2020 The MathWorks, Inc.\n",
      "                  R2020a (9.8.0.1323502) 64-bit (glnxa64)\n",
      "                             February 25, 2020\n",
      "\n",
      " \n",
      "To get started, type doc.\n",
      "For product information, visit www.mathworks.com.\n",
      " \n",
      "** CREATED new habit index file **\n",
      "\u001b[?1l\u001b>"
     ]
    }
   ],
   "source": [
    "# Create the habit index csv file (by running a dedicated matlab folder)\n",
    "!matlab -nodesktop -nosplash -r \"run('/export2/DATA/HIS/HIS_server/codes/create_HABIT_INDEX_table')\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare and arrange the behavioral data and DTI folder data for all together and for each group seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create subjects list folder for each group:\n",
    "subjFoldersDTI_short = [sub for sub in subjFoldersDTI if int(sub.split('-')[-1])<200]\n",
    "subjFoldersDTI_long = [sub for sub in subjFoldersDTI if int(sub.split('-')[-1])>200]\n",
    "\n",
    "# Get the behavioral data:\n",
    "behav_all = pd.read_csv(os.path.join(behav_data_path, 'habitIndex_ALL.csv'))\n",
    "# remove subjects that don't have DTI data\n",
    "absent_from_DTI_data = [sub for sub in behav_all.subID if 'sub-' + str(sub) not in subjFoldersDTI]\n",
    "behav_all = behav_all[~behav_all.subID.isin(absent_from_DTI_data)]\n",
    "# now make sure that subject lists are matching:\n",
    "if not ['sub-' + str(sub) for sub in behav_all.subID] == subjFoldersDTI:\n",
    "    raise Exception('Subject lists are not matching')\n",
    "\n",
    "# Repeat for the long training groups:\n",
    "behav_long = pd.read_csv(os.path.join(behav_data_path, 'habitIndex_LONG.csv'))\n",
    "# remove subjects that don't have DTI data\n",
    "absent_from_DTI_data = [sub for sub in behav_long.subID if 'sub-' + str(sub) not in subjFoldersDTI_long]\n",
    "behav_long = behav_long[~behav_long.subID.isin(absent_from_DTI_data)]\n",
    "# now make sure that subject lists are matching:\n",
    "if not ['sub-' + str(sub) for sub in behav_long.subID] == subjFoldersDTI_long:\n",
    "    raise Exception('Subject lists are not matching')\n",
    "    \n",
    "# Repeat for the short training groups:\n",
    "behav_short = pd.read_csv(os.path.join(behav_data_path, 'habitIndex_SHORT.csv'))\n",
    "# remove subjects that don't have DTI data\n",
    "absent_from_DTI_data = [sub for sub in behav_short.subID if 'sub-' + str(sub) not in subjFoldersDTI_short]\n",
    "behav_short = behav_short[~behav_short.subID.isin(absent_from_DTI_data)]\n",
    "# now make sure that subject lists are matching:\n",
    "if not ['sub-' + str(sub) for sub in behav_short.subID] == subjFoldersDTI_short:\n",
    "    raise Exception('Subject lists are not matching')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare the design matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Create the individual differences design matrix\n",
      "Text2Vest /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/individual_differences/design_indivdual_diff_LONG_temp.mat /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/individual_differences/design_indivdual_diff_LONG.mat\n",
      "Text2Vest /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/individual_differences/design_indivdual_diff_SHORT_temp.mat /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/individual_differences/design_indivdual_diff_SHORT.mat\n",
      "Text2Vest /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/individual_differences/design_indivdual_diff_temp.mat /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/individual_differences/design_indivdual_diff.con\n"
     ]
    }
   ],
   "source": [
    "print('> Create the individual differences design matrix')\n",
    "# define file names:\n",
    "design_indiv_diff_LONG = os.path.join(models_path, 'individual_differences', 'design_indivdual_diff_LONG.mat')\n",
    "design_indiv_diff_SHORT = os.path.join(models_path, 'individual_differences', 'design_indivdual_diff_SHORT.mat')\n",
    "contrasts_indiv_diff = os.path.join(models_path, 'individual_differences', 'design_indivdual_diff.con')\n",
    "\n",
    "# create the folder\n",
    "try:\n",
    "    os.makedirs(os.path.join(models_path, 'individual_differences'), exist_ok=False)\n",
    "    print('>> Created folder: ' + os.path.join(models_path, 'individual_differences'))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# write the design matrix files:\n",
    "behav_long.habit_index_mean_centered.to_csv(f\"{design_indiv_diff_LONG.split('.')[0]}_temp.mat\", index=False, header=False)\n",
    "behav_short.habit_index_mean_centered.to_csv(f\"{design_indiv_diff_SHORT.split('.')[0]}_temp.mat\", index=False, header=False)\n",
    "\n",
    "# write the contrast file:\n",
    "with open(f\"{contrasts_indiv_diff.split('.')[0]}_temp.mat\", 'w') as f:\n",
    "    f.write('1\\n-1\\n')\n",
    "\n",
    "# use the fsl command Text2Vect to make the design files good for randomise:\n",
    "print(f\"Text2Vest {design_indiv_diff_LONG.split('.')[0]}_temp.mat {design_indiv_diff_LONG}\")\n",
    "os.system(f\"Text2Vest {design_indiv_diff_LONG.split('.')[0]}_temp.mat {design_indiv_diff_LONG}\")\n",
    "print(f\"Text2Vest {design_indiv_diff_SHORT.split('.')[0]}_temp.mat {design_indiv_diff_SHORT}\")\n",
    "os.system(f\"Text2Vest {design_indiv_diff_SHORT.split('.')[0]}_temp.mat {design_indiv_diff_SHORT}\")\n",
    "print(f\"Text2Vest {contrasts_indiv_diff.split('.')[0]}_temp.mat {contrasts_indiv_diff}\")\n",
    "os.system(f\"Text2Vest {contrasts_indiv_diff.split('.')[0]}_temp.mat {contrasts_indiv_diff}\")\n",
    "\n",
    "# finally, remove the temporary files:\n",
    "os.remove(f\"{design_indiv_diff_LONG.split('.')[0]}_temp.mat\")\n",
    "os.remove(f\"{design_indiv_diff_SHORT.split('.')[0]}_temp.mat\")\n",
    "os.remove(f\"{contrasts_indiv_diff.split('.')[0]}_temp.mat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create one file with each group's participants together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for map_type in ['MD','FA']:          \n",
    "    print(f\"fslmerge -t {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f'NOTOPUP_EXCLUDED_LONG_individualDiff_AFTER_minus_BEFORE_{map_type}')} {' '.join([os.path.join(stats_path, map_type, f'{sub}_in-MNI_{map_type}_AFTER-minus-BEFORE.nii.gz') for sub in subjFoldersDTI_long])}\")\n",
    "    os.system(f\"fslmerge -t {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f'NOTOPUP_EXCLUDED_LONG_individualDiff_AFTER_minus_BEFORE_{map_type}')} {' '.join([os.path.join(stats_path, map_type, f'{sub}_in-MNI_{map_type}_AFTER-minus-BEFORE.nii.gz') for sub in subjFoldersDTI_long])}\")\n",
    "\n",
    "    print(f\"fslmerge -t {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f'NOTOPUP_EXCLUDED_SHORT_individualDiff_AFTER_minus_BEFORE_{map_type}')} {' '.join([os.path.join(stats_path, map_type, f'{sub}_in-MNI_{map_type}_AFTER-minus-BEFORE.nii.gz') for sub in subjFoldersDTI_short])}\")\n",
    "    os.system(f\"fslmerge -t {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f'NOTOPUP_EXCLUDED_SHORT_individualDiff_AFTER_minus_BEFORE_{map_type}')} {' '.join([os.path.join(stats_path, map_type, f'{sub}_in-MNI_{map_type}_AFTER-minus-BEFORE.nii.gz') for sub in subjFoldersDTI_short])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to run randomise for the individual differences analyses:\n",
    "def runRandomiseIndividualDiffs(group, map_type, region):\n",
    "    print(f'Running randomise for individual differences:\\n--- {group} training group | {map_type} | {region} ---\\n')\n",
    "    #print(f'MAKE SURE TO REPLACE IN THE FUNCTION THE the print to os.system before running the command !!!\\n')\n",
    "    os.system(f\"randomise_parallel \\\n",
    "        -i {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f'NOTOPUP_EXCLUDED_{group}_individualDiff_AFTER_minus_BEFORE_{map_type}')} \\\n",
    "        -o {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f'NOTOPUP_EXCLUDED_{group}_individualDiff_AFTER_minus_BEFORE_{map_type}_{region}')} \\\n",
    "        {'' if region=='whole_brain' else f'-m {masks_paths[region]}'} \\\n",
    "        -d {os.path.join(stats_path_NOTOPUP_EXCLUDED, 'models', 'individual_differences', f'design_indivdual_diff_{group}.mat')} \\\n",
    "        -t {os.path.join(stats_path_NOTOPUP_EXCLUDED, 'models', 'individual_differences', 'design_indivdual_diff.con')}  \\\n",
    "        -n 6000 -C 3.1 -D\")\n",
    "\n",
    "def runRandomiseIndividualDiffs_ALL_TERMINAL(groups, map_types, regions):\n",
    "    commandList = []\n",
    "    for group in groups:\n",
    "        for map_type in map_types:\n",
    "            for region in regions:\n",
    "                print(f'Running randomise for individual differences:\\n--- {group} training group | {map_type} | {region} ---\\n')\n",
    "                commandList.append(f\"randomise_parallel \\\n",
    "                    -i {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f'NOTOPUP_EXCLUDED_{group}_individualDiff_AFTER_minus_BEFORE_{map_type}')} \\\n",
    "                    -o {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f'NOTOPUP_EXCLUDED_{group}_individualDiff_AFTER_minus_BEFORE_{map_type}_{region}')} \\\n",
    "                    {'' if region=='whole_brain' else f'-m {masks_paths[region]}'} \\\n",
    "                    -d {os.path.join(stats_path_NOTOPUP_EXCLUDED, 'models', 'individual_differences', f'design_indivdual_diff_{group}.mat')} \\\n",
    "                    -t {os.path.join(stats_path_NOTOPUP_EXCLUDED, 'models', 'individual_differences', 'design_indivdual_diff.con')}  \\\n",
    "                    -n 6000 -C 3.1 -D && \")\n",
    "    commandList[-1] = commandList[-1][:-3]\n",
    "    return ''.join(commandList)\n",
    "\n",
    "\n",
    "# Get cluster info:\n",
    "def getClusterInfoIndividualDiffs(group, map_type, region, ReversedSigThresh='0.90'):\n",
    "    print(f'Test for POSITIVE relationship between:\\n--- increase in {map_type} and GOAL-DIRECTEDNESS in: {region} | group: {group} ---\\n')\n",
    "    print(os.popen(f'cluster \\\n",
    "        -i {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f\"NOTOPUP_EXCLUDED_{group}_individualDiff_AFTER_minus_BEFORE_{map_type}_{region}_clusterm_corrp_tstat1.nii.gz\")} \\\n",
    "        -c {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f\"NOTOPUP_EXCLUDED_{group}_individualDiff_AFTER_minus_BEFORE_{map_type}_{region}_tstat1.nii.gz\")}\\\n",
    "        -t {ReversedSigThresh} \\\n",
    "        --scalarname=corrp' + '\"1-p\"').read())\n",
    "\n",
    "    print(f'Test for NEGATIVE relationship between:\\n--- increase in {map_type} and GOAL-DIRECTEDNESS in: {region} | group: {group} ---\\n')\n",
    "    print(os.popen(f'cluster \\\n",
    "        -i {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f\"NOTOPUP_EXCLUDED_{group}_individualDiff_AFTER_minus_BEFORE_{map_type}_{region}_clusterm_corrp_tstat2.nii.gz\")} \\\n",
    "        -c {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f\"NOTOPUP_EXCLUDED_{group}_individualDiff_AFTER_minus_BEFORE_{map_type}_{region}_tstat2.nii.gz\")}\\\n",
    "        -t {ReversedSigThresh} \\\n",
    "        --scalarname=corrp' + '\"1-p\"').read())\n",
    "\n",
    "# Create a binary mask for the clusters:\n",
    "def createBinaryMaskIndividualDiffs(group, map_type, region, contrast, ReversedSigThresh='0.95'):\n",
    "    print(f'Create a binary mask for the cluster:\\n--- contrast {contrast} in: {map_type} | {region} | group: {group} ---\\n')\n",
    "    print(f'fslmaths {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f\"NOTOPUP_EXCLUDED_{group}_individualDiff_AFTER_minus_BEFORE_{map_type}_{region}_clusterm_corrp_tstat{contrast}.nii.gz\")} \\\n",
    "        -thr {ReversedSigThresh} -bin {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f\"NOTOPUP_EXCLUDED_{group}_individualDiff_AFTER_minus_BEFORE_{map_type}_{region}_contrast_{contrast}_bin_mask.nii.gz\")}')\n",
    "    os.system(f'fslmaths {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f\"NOTOPUP_EXCLUDED_{group}_individualDiff_AFTER_minus_BEFORE_{map_type}_{region}_clusterm_corrp_tstat{contrast}.nii.gz\")} \\\n",
    "        -thr {ReversedSigThresh} -bin {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f\"NOTOPUP_EXCLUDED_{group}_individualDiff_AFTER_minus_BEFORE_{map_type}_{region}_contrast_{contrast}_bin_mask.nii.gz\")}')\n",
    "    # create also a t-stat of the significant cluster:\n",
    "    print(f'fslmaths {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f\"NOTOPUP_EXCLUDED_{group}_individualDiff_AFTER_minus_BEFORE_{map_type}_{region}_tstat{contrast}.nii.gz\")} \\\n",
    "        -mul {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f\"NOTOPUP_EXCLUDED_{group}_individualDiff_AFTER_minus_BEFORE_{map_type}_{region}_contrast_{contrast}_bin_mask.nii.gz\")} \\\n",
    "            {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f\"NOTOPUP_EXCLUDED_{group}_individualDiff_AFTER_minus_BEFORE_{map_type}_{region}_tstat{contrast}_in_sig_cluster.nii.gz\")}')\n",
    "    os.system(f'fslmaths {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f\"NOTOPUP_EXCLUDED_{group}_individualDiff_AFTER_minus_BEFORE_{map_type}_{region}_tstat{contrast}.nii.gz\")} \\\n",
    "        -mul {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f\"NOTOPUP_EXCLUDED_{group}_individualDiff_AFTER_minus_BEFORE_{map_type}_{region}_contrast_{contrast}_bin_mask.nii.gz\")} \\\n",
    "            {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f\"NOTOPUP_EXCLUDED_{group}_individualDiff_AFTER_minus_BEFORE_{map_type}_{region}_tstat{contrast}_in_sig_cluster.nii.gz\")}')\n",
    "\n",
    "\n",
    "# get mean value of the cluster using a mask:\n",
    "def getClusterMeans(group, map_type, region, contrast):\n",
    "    print(f'Get mean value of a cluster:\\n--- contrast {contrast} in: {map_type} | {region} | group: {group} ---\\n')\n",
    "    print(f'fslstats -t {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f\"NOTOPUP_EXCLUDED_{group}_individualDiff_AFTER_minus_BEFORE_{map_type}.nii.gz\")} \\\n",
    "        -k {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f\"NOTOPUP_EXCLUDED_{group}_individualDiff_AFTER_minus_BEFORE_{map_type}_{region}_contrast_{contrast}_bin_mask.nii.gz\")} -M')    \n",
    "    avgs=os.popen(f'fslstats -t {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f\"NOTOPUP_EXCLUDED_{group}_individualDiff_AFTER_minus_BEFORE_{map_type}.nii.gz\")} \\\n",
    "        -k {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f\"NOTOPUP_EXCLUDED_{group}_individualDiff_AFTER_minus_BEFORE_{map_type}_{region}_contrast_{contrast}_bin_mask.nii.gz\")} -M').read()\n",
    "    return [float(i) for i in avgs.split(' \\n')[:-1]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run in terminal and summarize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the command to the terminal to run all the between group randomise analyses:\n",
    "command=runRandomiseIndividualDiffs_ALL_TERMINAL(groups=['SHORT', 'LONG'], map_types=['FA', 'MD'], regions=['whole_brain', 'Putamen', 'Caudate', 'vmPFC'])\n",
    "print(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for POSITIVE relationship between:\n",
      "--- increase in FA and GOAL-DIRECTEDNESS in: whole_brain | group: SHORT ---\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test for NEGATIVE relationship between:\n",
      "--- increase in FA and GOAL-DIRECTEDNESS in: whole_brain | group: SHORT ---\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test for POSITIVE relationship between:\n",
      "--- increase in FA and GOAL-DIRECTEDNESS in: Putamen | group: SHORT ---\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test for NEGATIVE relationship between:\n",
      "--- increase in FA and GOAL-DIRECTEDNESS in: Putamen | group: SHORT ---\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test for POSITIVE relationship between:\n",
      "--- increase in FA and GOAL-DIRECTEDNESS in: Caudate | group: SHORT ---\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test for NEGATIVE relationship between:\n",
      "--- increase in FA and GOAL-DIRECTEDNESS in: Caudate | group: SHORT ---\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test for POSITIVE relationship between:\n",
      "--- increase in FA and GOAL-DIRECTEDNESS in: vmPFC | group: SHORT ---\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test for NEGATIVE relationship between:\n",
      "--- increase in FA and GOAL-DIRECTEDNESS in: vmPFC | group: SHORT ---\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test for POSITIVE relationship between:\n",
      "--- increase in MD and GOAL-DIRECTEDNESS in: whole_brain | group: SHORT ---\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test for NEGATIVE relationship between:\n",
      "--- increase in MD and GOAL-DIRECTEDNESS in: whole_brain | group: SHORT ---\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test for POSITIVE relationship between:\n",
      "--- increase in MD and GOAL-DIRECTEDNESS in: Putamen | group: SHORT ---\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test for NEGATIVE relationship between:\n",
      "--- increase in MD and GOAL-DIRECTEDNESS in: Putamen | group: SHORT ---\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test for POSITIVE relationship between:\n",
      "--- increase in MD and GOAL-DIRECTEDNESS in: Caudate | group: SHORT ---\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test for NEGATIVE relationship between:\n",
      "--- increase in MD and GOAL-DIRECTEDNESS in: Caudate | group: SHORT ---\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test for POSITIVE relationship between:\n",
      "--- increase in MD and GOAL-DIRECTEDNESS in: vmPFC | group: SHORT ---\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test for NEGATIVE relationship between:\n",
      "--- increase in MD and GOAL-DIRECTEDNESS in: vmPFC | group: SHORT ---\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test for POSITIVE relationship between:\n",
      "--- increase in FA and GOAL-DIRECTEDNESS in: whole_brain | group: LONG ---\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test for NEGATIVE relationship between:\n",
      "--- increase in FA and GOAL-DIRECTEDNESS in: whole_brain | group: LONG ---\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test for POSITIVE relationship between:\n",
      "--- increase in FA and GOAL-DIRECTEDNESS in: Putamen | group: LONG ---\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "1\t10\t0.916\t33\t67\t35\t33.4\t68.2\t35.5\t4.06\t33\t68\t36\t3.57\n",
      "\n",
      "Test for NEGATIVE relationship between:\n",
      "--- increase in FA and GOAL-DIRECTEDNESS in: Putamen | group: LONG ---\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test for POSITIVE relationship between:\n",
      "--- increase in FA and GOAL-DIRECTEDNESS in: Caudate | group: LONG ---\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test for NEGATIVE relationship between:\n",
      "--- increase in FA and GOAL-DIRECTEDNESS in: Caudate | group: LONG ---\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test for POSITIVE relationship between:\n",
      "--- increase in FA and GOAL-DIRECTEDNESS in: vmPFC | group: LONG ---\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "1\t23\t0.909\t48\t96\t30\t49.2\t95.3\t31.7\t4.12\t49\t95\t32\t3.52\n",
      "\n",
      "Test for NEGATIVE relationship between:\n",
      "--- increase in FA and GOAL-DIRECTEDNESS in: vmPFC | group: LONG ---\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test for POSITIVE relationship between:\n",
      "--- increase in MD and GOAL-DIRECTEDNESS in: whole_brain | group: LONG ---\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test for NEGATIVE relationship between:\n",
      "--- increase in MD and GOAL-DIRECTEDNESS in: whole_brain | group: LONG ---\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test for POSITIVE relationship between:\n",
      "--- increase in MD and GOAL-DIRECTEDNESS in: Putamen | group: LONG ---\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test for NEGATIVE relationship between:\n",
      "--- increase in MD and GOAL-DIRECTEDNESS in: Putamen | group: LONG ---\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test for POSITIVE relationship between:\n",
      "--- increase in MD and GOAL-DIRECTEDNESS in: Caudate | group: LONG ---\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test for NEGATIVE relationship between:\n",
      "--- increase in MD and GOAL-DIRECTEDNESS in: Caudate | group: LONG ---\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test for POSITIVE relationship between:\n",
      "--- increase in MD and GOAL-DIRECTEDNESS in: vmPFC | group: LONG ---\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test for NEGATIVE relationship between:\n",
      "--- increase in MD and GOAL-DIRECTEDNESS in: vmPFC | group: LONG ---\n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the cluster info for the individual differences analysis:\n",
    "for group in ['SHORT', 'LONG']:\n",
    "    for map_type in ['FA', 'MD']:\n",
    "        for region in ['whole_brain', 'Putamen', 'Caudate', 'vmPFC']:\n",
    "            getClusterInfoIndividualDiffs(group, map_type, region, ReversedSigThresh='0.90')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics - Exploratory SUBGROUPS analysis (subgroups of habitual and goal-directed participants are inferred based on the behavioral measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the behavioral data:\n",
    "behav_clustered_all = pd.read_csv(os.path.join(behav_data_path, 'clustered_subgroups_HIS_May_2022.csv'))\n",
    "# remove subjects that don't have DTI data\n",
    "absent_from_DTI_data2 = [sub for sub in behav_clustered_all.ID if 'sub-' + str(sub) not in subjFoldersDTI]\n",
    "behav_clustered_all = behav_clustered_all[~behav_clustered_all.ID.isin(absent_from_DTI_data2)]\n",
    "\n",
    "# remove subjects that were excluded from the behavioral sub-groups clusters:\n",
    "absent_from_behav_clusters = [sub for sub in subjFoldersDTI if int(sub.split('-')[1]) not in list(behav_clustered_all.ID)]\n",
    "subjFoldersDTI_forSubgroups = [sub for sub in subjFoldersDTI if sub not in absent_from_behav_clusters]\n",
    "\n",
    "# now make sure that subject lists are matching:\n",
    "if not ['sub-' + str(sub) for sub in behav_clustered_all.ID] == subjFoldersDTI_forSubgroups:\n",
    "    raise Exception('Subject lists are not matching')\n",
    "    #pass # for debugging\n",
    "\n",
    "# create variables foreach group seperately and for the subgroups seperately:\n",
    "behav_clustered_short = behav_clustered_all[behav_clustered_all.ID < 200]\n",
    "behav_clustered_long = behav_clustered_all[behav_clustered_all.ID > 200]\n",
    "behav_clustered_goal_directed = behav_clustered_all[behav_clustered_all.Cluster == 'Goal-directed']\n",
    "behav_clustered_habitual = behav_clustered_all[behav_clustered_all.Cluster == 'Habitual']\n",
    "\n",
    "# sort the data according to the subgroups and create matching lists for the DTI data:\n",
    "behav_clustered_short = behav_clustered_short.sort_values(by=['Cluster', 'ID']).reset_index(drop=True)\n",
    "subjFoldersDTI_clustered_short = ['sub-' + str(sub) for sub in behav_clustered_short.ID]\n",
    "\n",
    "behav_clustered_long = behav_clustered_long.sort_values(by=['Cluster', 'ID']).reset_index(drop=True)\n",
    "subjFoldersDTI_clustered_long = ['sub-' + str(sub) for sub in behav_clustered_long.ID]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjFoldersDTI_clustered_short #goal-directed vs habitual - from behav_clustered_short\n",
    "subjFoldersDTI_clustered_long #goal-directed vs habitual - from behav_clustered_long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare the design matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('> Create the between group design matrix (of the calculated differences) - same is unpaired t-test (on the after-before differences)')\n",
    "# create the folder\n",
    "try:\n",
    "    os.makedirs(os.path.join(models_path, 'sub_groups'), exist_ok=False)\n",
    "    print('>> Created folder: ' + os.path.join(models_path, 'sub_groups'))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# create the design matrix and contrasts:\n",
    "N_goal_directed_in_clustered_1_DAY = behav_clustered_short.Cluster.value_counts()['Goal-directed']\n",
    "N_habitual_in_clustered_1_DAY = behav_clustered_short.Cluster.value_counts()['Habitual']\n",
    "print(f\"design_ttest2 {os.path.join(models_path, 'sub_groups','design_unpaired_ttest_subgroups_in_1_Day')} {N_goal_directed_in_clustered_1_DAY} {N_habitual_in_clustered_1_DAY}\")\n",
    "os.system(f\"design_ttest2 {os.path.join(models_path, 'sub_groups','design_unpaired_ttest_subgroups_in_1_Day')} {N_goal_directed_in_clustered_1_DAY} {N_habitual_in_clustered_1_DAY}\")\n",
    "\n",
    "N_goal_directed_in_clustered_3_DAY = behav_clustered_long.Cluster.value_counts()['Goal-directed']\n",
    "N_habitual_in_clustered_3_DAY = behav_clustered_long.Cluster.value_counts()['Habitual']\n",
    "print(f\"design_ttest2 {os.path.join(models_path, 'sub_groups','design_unpaired_ttest_subgroups_in_3_Day')} {N_goal_directed_in_clustered_3_DAY} {N_habitual_in_clustered_3_DAY}\")\n",
    "os.system(f\"design_ttest2 {os.path.join(models_path, 'sub_groups','design_unpaired_ttest_subgroups_in_3_Day')} {N_goal_directed_in_clustered_3_DAY} {N_habitual_in_clustered_3_DAY}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create one file with each group's participants together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for map_type in ['MD','FA']:          \n",
    "    print(f\"fslmerge -t {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f'NOTOPUP_EXCLUDED_SUBGROUPS_in_1_Day_AFTER_minus_BEFORE_{map_type}')} {' '.join([os.path.join(stats_path, map_type, f'{sub}_in-MNI_{map_type}_AFTER-minus-BEFORE.nii.gz') for sub in subjFoldersDTI_clustered_short])}\")\n",
    "    os.system(f\"fslmerge -t {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f'NOTOPUP_EXCLUDED_SUBGROUPS_in_1_Day_AFTER_minus_BEFORE_{map_type}')} {' '.join([os.path.join(stats_path, map_type, f'{sub}_in-MNI_{map_type}_AFTER-minus-BEFORE.nii.gz') for sub in subjFoldersDTI_clustered_short])}\")\n",
    "\n",
    "    print(f\"fslmerge -t {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f'NOTOPUP_EXCLUDED_SUBGROUPS_in_3_Day_AFTER_minus_BEFORE_{map_type}')} {' '.join([os.path.join(stats_path, map_type, f'{sub}_in-MNI_{map_type}_AFTER-minus-BEFORE.nii.gz') for sub in subjFoldersDTI_clustered_long])}\")\n",
    "    os.system(f\"fslmerge -t {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f'NOTOPUP_EXCLUDED_SUBGROUPS_in_3_Day_AFTER_minus_BEFORE_{map_type}')} {' '.join([os.path.join(stats_path, map_type, f'{sub}_in-MNI_{map_type}_AFTER-minus-BEFORE.nii.gz') for sub in subjFoldersDTI_clustered_long])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to run randomise for the individual differences analyses:\n",
    "def runRandomiseSubgroups(subgroups_in, map_type, region):\n",
    "    print(f'Running randomise for sub-groups analysis:\\n--- Subgroups in {subgroups_in} | {map_type} | {region} ---\\n')\n",
    "    #print(f'MAKE SURE TO REPLACE IN THE FUNCTION THE the print to os.system before running the command !!!\\n')\n",
    "    os.system(f\"randomise_parallel \\\n",
    "        -i {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f'NOTOPUP_EXCLUDED_SUBGROUPS_in_{subgroups_in}_AFTER_minus_BEFORE_{map_type}')} \\\n",
    "        -o {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f'NOTOPUP_EXCLUDED_SUBGROUPS_in_{subgroups_in}_AFTER_minus_BEFORE_{map_type}_{region}')} \\\n",
    "        {'' if region=='whole_brain' else f'-m {masks_paths[region]}'} \\\n",
    "        -d {os.path.join(stats_path_NOTOPUP_EXCLUDED, 'models', 'sub_groups', f'design_unpaired_ttest_subgroups_in_{subgroups_in}.mat')} \\\n",
    "        -t {os.path.join(stats_path_NOTOPUP_EXCLUDED, 'models', 'sub_groups', f'design_unpaired_ttest_subgroups_in_{subgroups_in}.con')}  \\\n",
    "        -n 6000 -C 3.1\")\n",
    "\n",
    "def runRandomiseSubgroups_ALL_TERMINAL(subgroups_optionalGroups, map_types, regions):\n",
    "    commandList = []\n",
    "    for subgroups_in in subgroups_optionalGroups:\n",
    "        for map_type in map_types:\n",
    "            for region in regions:\n",
    "                print(f'Running randomise for sub-groups analysis:\\n--- Subgroups in {subgroups_in} | {map_type} | {region} ---\\n')\n",
    "                commandList.append(f\"randomise_parallel \\\n",
    "                    -i {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f'NOTOPUP_EXCLUDED_SUBGROUPS_in_{subgroups_in}_AFTER_minus_BEFORE_{map_type}')} \\\n",
    "                    -o {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f'NOTOPUP_EXCLUDED_SUBGROUPS_in_{subgroups_in}_AFTER_minus_BEFORE_{map_type}_{region}')} \\\n",
    "                    {'' if region=='whole_brain' else f'-m {masks_paths[region]}'} \\\n",
    "                    -d {os.path.join(stats_path_NOTOPUP_EXCLUDED, 'models', 'sub_groups', f'design_unpaired_ttest_subgroups_in_{subgroups_in}.mat')} \\\n",
    "                    -t {os.path.join(stats_path_NOTOPUP_EXCLUDED, 'models', 'sub_groups', f'design_unpaired_ttest_subgroups_in_{subgroups_in}.con')}  \\\n",
    "                    -n 6000 -C 3.1 && \")\n",
    "    commandList[-1] = commandList[-1][:-3]\n",
    "    return ''.join(commandList)\n",
    "\n",
    "\n",
    "# Get cluster info:\n",
    "def getClusterInfoSubgroups(subgroups_in, map_type, region, ReversedSigThresh='0.90'):\n",
    "    print(f'Testing stats for sub-groups analysis:\\n--- Subgroups in {subgroups_in} | {map_type} | {region} ---\\n')\n",
    "    print(f'Goal-directed > Test Habitual \\n' if subgroups_in in ['1_Day','3_Day'] else f'1-Day > 3-Day\\n')\n",
    "    print(os.popen(f'cluster \\\n",
    "        -i {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f\"NOTOPUP_EXCLUDED_SUBGROUPS_in_{subgroups_in}_AFTER_minus_BEFORE_{map_type}_{region}_clusterm_corrp_tstat1.nii.gz\")} \\\n",
    "        -c {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f\"NOTOPUP_EXCLUDED_SUBGROUPS_in_{subgroups_in}_AFTER_minus_BEFORE_{map_type}_{region}_tstat1.nii.gz\")}\\\n",
    "        -t {ReversedSigThresh} \\\n",
    "        --scalarname=corrp' + '\"1-p\"').read())\n",
    "\n",
    "    print(f'Test Habitual > Goal-Directed \\n' if subgroups_in in ['1_Day','3_Day'] else f'3-Day > 1-Day\\n')\n",
    "    print(os.popen(f'cluster \\\n",
    "        -i {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f\"NOTOPUP_EXCLUDED_SUBGROUPS_in_{subgroups_in}_AFTER_minus_BEFORE_{map_type}_{region}_clusterm_corrp_tstat2.nii.gz\")} \\\n",
    "        -c {os.path.join(stats_path_NOTOPUP_EXCLUDED, map_type, f\"NOTOPUP_EXCLUDED_SUBGROUPS_in_{subgroups_in}_AFTER_minus_BEFORE_{map_type}_{region}_tstat2.nii.gz\")}\\\n",
    "        -t {ReversedSigThresh} \\\n",
    "        --scalarname=corrp' + '\"1-p\"').read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run in terminal and summarize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running randomise for sub-groups analysis:\n",
      "--- Subgroups in 1_Day | FA | whole_brain ---\n",
      "\n",
      "Running randomise for sub-groups analysis:\n",
      "--- Subgroups in 1_Day | FA | Putamen ---\n",
      "\n",
      "Running randomise for sub-groups analysis:\n",
      "--- Subgroups in 1_Day | FA | Caudate ---\n",
      "\n",
      "Running randomise for sub-groups analysis:\n",
      "--- Subgroups in 1_Day | FA | vmPFC ---\n",
      "\n",
      "Running randomise for sub-groups analysis:\n",
      "--- Subgroups in 1_Day | MD | whole_brain ---\n",
      "\n",
      "Running randomise for sub-groups analysis:\n",
      "--- Subgroups in 1_Day | MD | Putamen ---\n",
      "\n",
      "Running randomise for sub-groups analysis:\n",
      "--- Subgroups in 1_Day | MD | Caudate ---\n",
      "\n",
      "Running randomise for sub-groups analysis:\n",
      "--- Subgroups in 1_Day | MD | vmPFC ---\n",
      "\n",
      "Running randomise for sub-groups analysis:\n",
      "--- Subgroups in 3_Day | FA | whole_brain ---\n",
      "\n",
      "Running randomise for sub-groups analysis:\n",
      "--- Subgroups in 3_Day | FA | Putamen ---\n",
      "\n",
      "Running randomise for sub-groups analysis:\n",
      "--- Subgroups in 3_Day | FA | Caudate ---\n",
      "\n",
      "Running randomise for sub-groups analysis:\n",
      "--- Subgroups in 3_Day | FA | vmPFC ---\n",
      "\n",
      "Running randomise for sub-groups analysis:\n",
      "--- Subgroups in 3_Day | MD | whole_brain ---\n",
      "\n",
      "Running randomise for sub-groups analysis:\n",
      "--- Subgroups in 3_Day | MD | Putamen ---\n",
      "\n",
      "Running randomise for sub-groups analysis:\n",
      "--- Subgroups in 3_Day | MD | Caudate ---\n",
      "\n",
      "Running randomise for sub-groups analysis:\n",
      "--- Subgroups in 3_Day | MD | vmPFC ---\n",
      "\n",
      "randomise_parallel                     -i /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/FA/NOTOPUP_EXCLUDED_SUBGROUPS_in_1_Day_AFTER_minus_BEFORE_FA                     -o /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/FA/NOTOPUP_EXCLUDED_SUBGROUPS_in_1_Day_AFTER_minus_BEFORE_FA_whole_brain                                          -d /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/sub_groups/design_unpaired_ttest_subgroups_in_1_Day.mat                     -t /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/sub_groups/design_unpaired_ttest_subgroups_in_1_Day.con                      -n 6000 -C 3.1 && randomise_parallel                     -i /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/FA/NOTOPUP_EXCLUDED_SUBGROUPS_in_1_Day_AFTER_minus_BEFORE_FA                     -o /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/FA/NOTOPUP_EXCLUDED_SUBGROUPS_in_1_Day_AFTER_minus_BEFORE_FA_Putamen                     -m /export2/DATA/HIS/HIS_server/DTI_assitance_files/masks/Harvard-Oxford/Putamen-mask.nii.gz                     -d /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/sub_groups/design_unpaired_ttest_subgroups_in_1_Day.mat                     -t /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/sub_groups/design_unpaired_ttest_subgroups_in_1_Day.con                      -n 6000 -C 3.1 && randomise_parallel                     -i /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/FA/NOTOPUP_EXCLUDED_SUBGROUPS_in_1_Day_AFTER_minus_BEFORE_FA                     -o /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/FA/NOTOPUP_EXCLUDED_SUBGROUPS_in_1_Day_AFTER_minus_BEFORE_FA_Caudate                     -m /export2/DATA/HIS/HIS_server/DTI_assitance_files/masks/Harvard-Oxford/CaudateHead_Y-larger-than-1-mask.nii.gz                     -d /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/sub_groups/design_unpaired_ttest_subgroups_in_1_Day.mat                     -t /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/sub_groups/design_unpaired_ttest_subgroups_in_1_Day.con                      -n 6000 -C 3.1 && randomise_parallel                     -i /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/FA/NOTOPUP_EXCLUDED_SUBGROUPS_in_1_Day_AFTER_minus_BEFORE_FA                     -o /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/FA/NOTOPUP_EXCLUDED_SUBGROUPS_in_1_Day_AFTER_minus_BEFORE_FA_vmPFC                     -m /export2/DATA/HIS/HIS_server/DTI_assitance_files/masks/Harvard-Oxford/vmPFC-mask.nii.gz                     -d /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/sub_groups/design_unpaired_ttest_subgroups_in_1_Day.mat                     -t /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/sub_groups/design_unpaired_ttest_subgroups_in_1_Day.con                      -n 6000 -C 3.1 && randomise_parallel                     -i /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/MD/NOTOPUP_EXCLUDED_SUBGROUPS_in_1_Day_AFTER_minus_BEFORE_MD                     -o /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/MD/NOTOPUP_EXCLUDED_SUBGROUPS_in_1_Day_AFTER_minus_BEFORE_MD_whole_brain                                          -d /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/sub_groups/design_unpaired_ttest_subgroups_in_1_Day.mat                     -t /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/sub_groups/design_unpaired_ttest_subgroups_in_1_Day.con                      -n 6000 -C 3.1 && randomise_parallel                     -i /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/MD/NOTOPUP_EXCLUDED_SUBGROUPS_in_1_Day_AFTER_minus_BEFORE_MD                     -o /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/MD/NOTOPUP_EXCLUDED_SUBGROUPS_in_1_Day_AFTER_minus_BEFORE_MD_Putamen                     -m /export2/DATA/HIS/HIS_server/DTI_assitance_files/masks/Harvard-Oxford/Putamen-mask.nii.gz                     -d /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/sub_groups/design_unpaired_ttest_subgroups_in_1_Day.mat                     -t /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/sub_groups/design_unpaired_ttest_subgroups_in_1_Day.con                      -n 6000 -C 3.1 && randomise_parallel                     -i /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/MD/NOTOPUP_EXCLUDED_SUBGROUPS_in_1_Day_AFTER_minus_BEFORE_MD                     -o /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/MD/NOTOPUP_EXCLUDED_SUBGROUPS_in_1_Day_AFTER_minus_BEFORE_MD_Caudate                     -m /export2/DATA/HIS/HIS_server/DTI_assitance_files/masks/Harvard-Oxford/CaudateHead_Y-larger-than-1-mask.nii.gz                     -d /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/sub_groups/design_unpaired_ttest_subgroups_in_1_Day.mat                     -t /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/sub_groups/design_unpaired_ttest_subgroups_in_1_Day.con                      -n 6000 -C 3.1 && randomise_parallel                     -i /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/MD/NOTOPUP_EXCLUDED_SUBGROUPS_in_1_Day_AFTER_minus_BEFORE_MD                     -o /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/MD/NOTOPUP_EXCLUDED_SUBGROUPS_in_1_Day_AFTER_minus_BEFORE_MD_vmPFC                     -m /export2/DATA/HIS/HIS_server/DTI_assitance_files/masks/Harvard-Oxford/vmPFC-mask.nii.gz                     -d /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/sub_groups/design_unpaired_ttest_subgroups_in_1_Day.mat                     -t /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/sub_groups/design_unpaired_ttest_subgroups_in_1_Day.con                      -n 6000 -C 3.1 && randomise_parallel                     -i /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/FA/NOTOPUP_EXCLUDED_SUBGROUPS_in_3_Day_AFTER_minus_BEFORE_FA                     -o /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/FA/NOTOPUP_EXCLUDED_SUBGROUPS_in_3_Day_AFTER_minus_BEFORE_FA_whole_brain                                          -d /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/sub_groups/design_unpaired_ttest_subgroups_in_3_Day.mat                     -t /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/sub_groups/design_unpaired_ttest_subgroups_in_3_Day.con                      -n 6000 -C 3.1 && randomise_parallel                     -i /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/FA/NOTOPUP_EXCLUDED_SUBGROUPS_in_3_Day_AFTER_minus_BEFORE_FA                     -o /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/FA/NOTOPUP_EXCLUDED_SUBGROUPS_in_3_Day_AFTER_minus_BEFORE_FA_Putamen                     -m /export2/DATA/HIS/HIS_server/DTI_assitance_files/masks/Harvard-Oxford/Putamen-mask.nii.gz                     -d /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/sub_groups/design_unpaired_ttest_subgroups_in_3_Day.mat                     -t /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/sub_groups/design_unpaired_ttest_subgroups_in_3_Day.con                      -n 6000 -C 3.1 && randomise_parallel                     -i /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/FA/NOTOPUP_EXCLUDED_SUBGROUPS_in_3_Day_AFTER_minus_BEFORE_FA                     -o /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/FA/NOTOPUP_EXCLUDED_SUBGROUPS_in_3_Day_AFTER_minus_BEFORE_FA_Caudate                     -m /export2/DATA/HIS/HIS_server/DTI_assitance_files/masks/Harvard-Oxford/CaudateHead_Y-larger-than-1-mask.nii.gz                     -d /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/sub_groups/design_unpaired_ttest_subgroups_in_3_Day.mat                     -t /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/sub_groups/design_unpaired_ttest_subgroups_in_3_Day.con                      -n 6000 -C 3.1 && randomise_parallel                     -i /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/FA/NOTOPUP_EXCLUDED_SUBGROUPS_in_3_Day_AFTER_minus_BEFORE_FA                     -o /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/FA/NOTOPUP_EXCLUDED_SUBGROUPS_in_3_Day_AFTER_minus_BEFORE_FA_vmPFC                     -m /export2/DATA/HIS/HIS_server/DTI_assitance_files/masks/Harvard-Oxford/vmPFC-mask.nii.gz                     -d /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/sub_groups/design_unpaired_ttest_subgroups_in_3_Day.mat                     -t /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/sub_groups/design_unpaired_ttest_subgroups_in_3_Day.con                      -n 6000 -C 3.1 && randomise_parallel                     -i /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/MD/NOTOPUP_EXCLUDED_SUBGROUPS_in_3_Day_AFTER_minus_BEFORE_MD                     -o /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/MD/NOTOPUP_EXCLUDED_SUBGROUPS_in_3_Day_AFTER_minus_BEFORE_MD_whole_brain                                          -d /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/sub_groups/design_unpaired_ttest_subgroups_in_3_Day.mat                     -t /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/sub_groups/design_unpaired_ttest_subgroups_in_3_Day.con                      -n 6000 -C 3.1 && randomise_parallel                     -i /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/MD/NOTOPUP_EXCLUDED_SUBGROUPS_in_3_Day_AFTER_minus_BEFORE_MD                     -o /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/MD/NOTOPUP_EXCLUDED_SUBGROUPS_in_3_Day_AFTER_minus_BEFORE_MD_Putamen                     -m /export2/DATA/HIS/HIS_server/DTI_assitance_files/masks/Harvard-Oxford/Putamen-mask.nii.gz                     -d /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/sub_groups/design_unpaired_ttest_subgroups_in_3_Day.mat                     -t /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/sub_groups/design_unpaired_ttest_subgroups_in_3_Day.con                      -n 6000 -C 3.1 && randomise_parallel                     -i /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/MD/NOTOPUP_EXCLUDED_SUBGROUPS_in_3_Day_AFTER_minus_BEFORE_MD                     -o /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/MD/NOTOPUP_EXCLUDED_SUBGROUPS_in_3_Day_AFTER_minus_BEFORE_MD_Caudate                     -m /export2/DATA/HIS/HIS_server/DTI_assitance_files/masks/Harvard-Oxford/CaudateHead_Y-larger-than-1-mask.nii.gz                     -d /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/sub_groups/design_unpaired_ttest_subgroups_in_3_Day.mat                     -t /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/sub_groups/design_unpaired_ttest_subgroups_in_3_Day.con                      -n 6000 -C 3.1 && randomise_parallel                     -i /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/MD/NOTOPUP_EXCLUDED_SUBGROUPS_in_3_Day_AFTER_minus_BEFORE_MD                     -o /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/MD/NOTOPUP_EXCLUDED_SUBGROUPS_in_3_Day_AFTER_minus_BEFORE_MD_vmPFC                     -m /export2/DATA/HIS/HIS_server/DTI_assitance_files/masks/Harvard-Oxford/vmPFC-mask.nii.gz                     -d /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/sub_groups/design_unpaired_ttest_subgroups_in_3_Day.mat                     -t /export2/DATA/HIS/HIS_server/analysis/dwi_data/stats_alt_reg_NOTOPUP_EXCLUDED/models/sub_groups/design_unpaired_ttest_subgroups_in_3_Day.con                      -n 6000 -C 3.1 \n"
     ]
    }
   ],
   "source": [
    "command = runRandomiseSubgroups_ALL_TERMINAL(subgroups_optionalGroups=['1_Day', '3_Day'], map_types=['FA', 'MD'], regions=['whole_brain', 'Putamen', 'Caudate', 'vmPFC'])\n",
    "print(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing stats for sub-groups analysis:\n",
      "--- Subgroups in 1_Day | FA | whole_brain ---\n",
      "\n",
      "Goal-directed > Test Habitual \n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test Habitual > Goal-Directed \n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Testing stats for sub-groups analysis:\n",
      "--- Subgroups in 1_Day | FA | Putamen ---\n",
      "\n",
      "Goal-directed > Test Habitual \n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test Habitual > Goal-Directed \n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Testing stats for sub-groups analysis:\n",
      "--- Subgroups in 1_Day | FA | Caudate ---\n",
      "\n",
      "Goal-directed > Test Habitual \n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test Habitual > Goal-Directed \n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Testing stats for sub-groups analysis:\n",
      "--- Subgroups in 1_Day | FA | vmPFC ---\n",
      "\n",
      "Goal-directed > Test Habitual \n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test Habitual > Goal-Directed \n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Testing stats for sub-groups analysis:\n",
      "--- Subgroups in 1_Day | MD | whole_brain ---\n",
      "\n",
      "Goal-directed > Test Habitual \n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test Habitual > Goal-Directed \n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Testing stats for sub-groups analysis:\n",
      "--- Subgroups in 1_Day | MD | Putamen ---\n",
      "\n",
      "Goal-directed > Test Habitual \n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test Habitual > Goal-Directed \n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Testing stats for sub-groups analysis:\n",
      "--- Subgroups in 1_Day | MD | Caudate ---\n",
      "\n",
      "Goal-directed > Test Habitual \n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test Habitual > Goal-Directed \n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Testing stats for sub-groups analysis:\n",
      "--- Subgroups in 1_Day | MD | vmPFC ---\n",
      "\n",
      "Goal-directed > Test Habitual \n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test Habitual > Goal-Directed \n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Testing stats for sub-groups analysis:\n",
      "--- Subgroups in 3_Day | FA | whole_brain ---\n",
      "\n",
      "Goal-directed > Test Habitual \n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test Habitual > Goal-Directed \n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "1\t213\t0.96\t48\t53\t17\t44.4\t53\t24.8\t5.26\t39\t54\t27\t3.61\n",
      "\n",
      "Testing stats for sub-groups analysis:\n",
      "--- Subgroups in 3_Day | FA | Putamen ---\n",
      "\n",
      "Goal-directed > Test Habitual \n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test Habitual > Goal-Directed \n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Testing stats for sub-groups analysis:\n",
      "--- Subgroups in 3_Day | FA | Caudate ---\n",
      "\n",
      "Goal-directed > Test Habitual \n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test Habitual > Goal-Directed \n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Testing stats for sub-groups analysis:\n",
      "--- Subgroups in 3_Day | FA | vmPFC ---\n",
      "\n",
      "Goal-directed > Test Habitual \n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test Habitual > Goal-Directed \n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Testing stats for sub-groups analysis:\n",
      "--- Subgroups in 3_Day | MD | whole_brain ---\n",
      "\n",
      "Goal-directed > Test Habitual \n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test Habitual > Goal-Directed \n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Testing stats for sub-groups analysis:\n",
      "--- Subgroups in 3_Day | MD | Putamen ---\n",
      "\n",
      "Goal-directed > Test Habitual \n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test Habitual > Goal-Directed \n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Testing stats for sub-groups analysis:\n",
      "--- Subgroups in 3_Day | MD | Caudate ---\n",
      "\n",
      "Goal-directed > Test Habitual \n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test Habitual > Goal-Directed \n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "1\t6\t0.904\t38\t67\t45\t37.8\t67.2\t45.7\t3.41\t38\t67\t46\t3.26\n",
      "\n",
      "Testing stats for sub-groups analysis:\n",
      "--- Subgroups in 3_Day | MD | vmPFC ---\n",
      "\n",
      "Goal-directed > Test Habitual \n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n",
      "Test Habitual > Goal-Directed \n",
      "\n",
      "Cluster Index\tVoxels\tcorrp1-p-MAX\tcorrp1-p-MAX X (vox)\tcorrp1-p-MAX Y (vox)\tcorrp1-p-MAX Z (vox)\tcorrp1-p-COG X (vox)\tcorrp1-p-COG Y (vox)\tcorrp1-p-COG Z (vox)\tCOPE-MAX\tCOPE-MAX X (vox)\tCOPE-MAX Y (vox)\tCOPE-MAX Z (vox)\tCOPE-MEAN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get cluster info:\n",
    "for subgroups_in in ['1_Day', '3_Day']:\n",
    "    for map_type in ['FA', 'MD']:\n",
    "        for region in ['whole_brain', 'Putamen', 'Caudate', 'vmPFC']:\n",
    "            getClusterInfoSubgroups(subgroups_in, map_type, region, ReversedSigThresh='0.90')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
